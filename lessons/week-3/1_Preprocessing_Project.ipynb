{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Preprocessing – PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data\n",
    "\n",
    "Put your data in the `data` folder of this repo and replace `YOUR_FILE.csv` below with the name of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525,
     "status": "ok",
     "timestamp": 1647459522220,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "bIB1SsShP9iL",
    "outputId": "ab2ec235-c74b-4f81-f7dc-d41825605164"
   },
   "outputs": [],
   "source": [
    "# Import the pandas package\n",
    "import pandas as pd \n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv('../../data/YOUR_FILE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the shape, first rows, and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1647459542614,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "TkI-FuY2mwX6",
    "outputId": "87472d59-a42b-4a76-ff02-3aaa0f474e78",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1639763489064,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "7oZfTJi2rA6Z",
    "outputId": "88d7211b-3e6e-447c-87da-fcffadc71a58"
   },
   "outputs": [],
   "source": [
    "# This allows you to quickly see which columns you have\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns\n",
    "df = df.drop(['self', 'url', 'subreddit', 'augmented_at', 'augmented_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows that don't have 'removed' or 'deleted' as the selftext\n",
    "df = df.loc[~df['selftext'].isin(['[removed]', '[deleted]' ]),:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values in selftext\n",
    "df = df.dropna(subset=['selftext'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1647460151917,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qtrgcSZH4Rnb"
   },
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='selftext'):\n",
    "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
    "    for text in df[text_col]:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens\n",
    "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "                  else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjbgskrlDg16"
   },
   "outputs": [],
   "source": [
    "# This may take a while\n",
    "lemmas = [line for line in preprocess(df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63667,
     "status": "ok",
     "timestamp": 1639846696683,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "MxeZ7mc9DeMj",
    "outputId": "a478fee9-f5b1-4562-e3a6-860e244e5719"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Create bigram and trigram models\n",
    "bigram = Phrases(lemmas, min_count=10, threshold=100)\n",
    "trigram = Phrases(bigram[lemmas], min_count=10, threshold=50)  \n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = [trigram_phraser[bigram_phraser[doc]] for doc in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each into a string\n",
    "trigrams_joined = [' '.join(trigram) for trigram in trigrams]\n",
    "trigrams_joined[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many bigrams were identified by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1639846804300,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "xO4enOmLeXXi",
    "outputId": "921b6ce4-3390-4f90-ab13-e7bd4e0ce11f"
   },
   "outputs": [],
   "source": [
    "len(bigram_phraser.phrasegrams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first few bigrams identified in the model to check if they seem  appropriate. If not, you can play around with the parameters of the bigram model to adjust the sensitivity of the model (the values for `min_count` and `threshold` above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1639846859895,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "uwySeLM4df97",
    "outputId": "0940755f-397e-48cc-ecdb-3558341cb7ae"
   },
   "outputs": [],
   "source": [
    "list(bigram_phraser.phrasegrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at trigrams\n",
    "[trigram for trigram in list(trigram_phraser.phrasegrams.keys()) if trigram.count('_') == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVgem81TZWzu"
   },
   "source": [
    "## Saving data\n",
    "\n",
    "Add the new preprocessed data to our .csv in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBe1-7806O0A"
   },
   "outputs": [],
   "source": [
    "# Inserting next to selftext column\n",
    "df.insert(loc=7, column='lemmas', value=trigrams_joined)\n",
    "# Removing empty rows in lemmas\n",
    "df = df[~df['lemmas'].isin([''])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `YOUR_FILE` below to the name of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to new csv\n",
    "df.to_csv('YOUR_FILE_lemmas.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 1 Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
