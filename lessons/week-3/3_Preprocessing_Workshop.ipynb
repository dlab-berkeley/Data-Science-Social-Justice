{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Preprocessing\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand why textual data needs to be preprocessed.\n",
    "* Engage in common preprocessing tasks, such as lemmatization and phrase modeling.\n",
    "* Distinguish between different Python packages for preprocessing.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "\n",
    "### Sections\n",
    "1. [About This Workshop](#intro)\n",
    "2. [Reading Data with Pandas](#read)\n",
    "3. [Dropping Columns and Missing Values](#drop)\n",
    "4. [Preprocessing Text Data with SpaCy](#clean)\n",
    "5. [Phrase Modeling with Gensim](#gensim)\n",
    "6. [Saving Data](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36-_8ZvxmwXf"
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "# About This Workshop\n",
    "\n",
    "One of the most common types of data used in machine learning and data science is text data. This includes social media posts, novels, customer reviews, interview transcripts, and many others. Text is a powerful source of information, but requires specific preprocessing in order to be used in machine learning contexts. In particular, machine learning algorithms are designed to work with numerical data, so the end goal of text preprocessing is to have numeric features associated with each text in the dataset. \n",
    "\n",
    "In this notebook we will go over a preprocessing **pipeline** (or series of sequential steps) for text, which is the first step that we need to take in order to analyze text using our data science techniques.\n",
    "\n",
    "This notebook is designed to help you: \n",
    "\n",
    "1. Read, manipulate, and write `.csv` files in a `pandas` dataframe;\n",
    "2. Preprocess text with the following key skills: tokenizing, stop-word removal, n-grams extraction, and lemmatization using `spaCy`;\n",
    "\n",
    "In Python Fundamentals, you were introduced to the basics of the `pandas` package, which we will use extensively in this notebook. This module is designed to accelerate your coding skills so that you can use Python effectively for generating research results. However, if you have less experience with Python, some of this will be overwhelming. That's totally normal, since it takes more than a couple of hours to learn how to code! \n",
    "\n",
    "If you are feeling overwhelmed, it can be helpful to focus on the broader purpose of the functions in the notebook for now and how we can use them to further our research purposes, rather than the details of the algorithms. It can also help to remember that you don't need to memorize every function you want to use. Even programmers who have been working with Python for years will regularly refer to documentation and resources (like this notebook!) while they are developing a project. With time and experience as you progress through the notebooks you will get more comfortable with the Python code underlying this notebook.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Some of the code will take a long time to execute. The `*` on the left side of the cell will indicate that the cell is still running. The nice thing about preprocessing is that once we have the pipeline complete, we will save our results so we don't need to re-run these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgBDYstgmwX6"
   },
   "source": [
    "<a id='read'></a>\n",
    "\n",
    "# Recap: Our Data\n",
    "\n",
    "The first step is to read the data we are going to work with into Python so that we can work with it.\n",
    "\n",
    "We're using the dataset taken from the subreddit [r/amitheasshole](reddit.com/r/AmItheAsshole). \n",
    "\n",
    "The subreddit describes itself as follows:\n",
    "\n",
    "<img src=\"../../images/aita_desc.png\" alt=\"Am I The Asshole - description\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subreddit has structures in place that the community follow to come to a decision about the situation. First, OP (original poster) writes up the situation, asking AITA (Am I The Asshole). In response, for eighteen hours, the community of the subreddit will respond to the post with one of five judgments: YTA (You‚Äôre The Asshole), NTA (Not The Asshole), ESH (Everyone Sucks Here), NAH (No Assholes Here), or INFO (Not Enough Info).\n",
    "\n",
    "üí° **Tip**: For more info on the subreddit, see [here](https://www.inverse.com/culture/am-i-the-asshole/amp). \n",
    "\n",
    "First, we have to read the data. We'll use a subset of the full dataset consisting of the top most popular posts, the assumption being that this will yield the most interesting results (`aita_sub_top_sm.csv`). We use `pd.read_csv()` to import the .csv file as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525,
     "status": "ok",
     "timestamp": 1647459522220,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "bIB1SsShP9iL",
    "outputId": "ab2ec235-c74b-4f81-f7dc-d41825605164"
   },
   "outputs": [],
   "source": [
    "# Import the pandas package\n",
    "import pandas as pd \n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv('../../data/aita_top_submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at look and the shape, top rows, and columns of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1647459542614,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "TkI-FuY2mwX6",
    "outputId": "87472d59-a42b-4a76-ff02-3aaa0f474e78",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1639763489064,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "7oZfTJi2rA6Z",
    "outputId": "88d7211b-3e6e-447c-87da-fcffadc71a58"
   },
   "outputs": [],
   "source": [
    "# This allows you to quickly see which columns you have\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzrQAWgVqgwD"
   },
   "source": [
    "This particular dataset only includes the original posts in the subreddit (so not the comments on the posts). \n",
    "\n",
    "There is one row per post in the dataset. The columns are as follows:\n",
    "\n",
    "-  `idstr`: ID of the post.\n",
    "- `created`: the time of the post's creation.\n",
    "- `author`: Reddit author of the post.\n",
    "- `title`: Title of the post.\n",
    "- `selftext`: Text of the post.\n",
    "- `score`: Amount of upvotes minus downvotes.\n",
    "- `textlen`: Amount of words.\n",
    "- `num_comments`: Amount of comments.\n",
    "- `nsfw`: Flag for NSFW content.\n",
    "- `flair_text`: A 'tag' that users within a subreddit can add.\n",
    "\n",
    "üîî **Question**: Which of these columns could contain interesting data for your research purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaH8ZNOv50it"
   },
   "source": [
    "## Removing Missing Values\n",
    "\n",
    "First, we want to select the rows of deleted posts. On Reddit, removed posts get flagged as \"[removed]\" or \"[deleted]\". We should remove these posts, since they'll lack any text. \n",
    "\n",
    "We can do this using the [`isin()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html) method. We run it on the `selftext` column and include a list of phrases that indicate a removed post ‚Äì that is, \"[removed]\" and \"[deleted]\".\n",
    "\n",
    "The following line of code will select all lines that do not have 'removed' or 'deleted' in the post's text. The `~` means 'not'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16325, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all rows that don't have '[removed]' or '[deleted]'\n",
    "df = df.loc[~df['selftext'].isin(['[removed]', '[deleted]' ]),:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: How many posts are left in the dataset? How many did we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Null Values\n",
    "\n",
    "Next, we need to drop **null values**. These are values that are totally missing. In this case, the web scraper may have been unable to extract text for the post. They are replaced with the value **NaN**, which stands for a null value in `pandas`. We need to deal with null values in any column that we plan to use for analysis, which in this case is the `selftext` column. We can use [`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html?highlight=dropna#pandas.DataFrame.dropna) to remove the rows with null values in the target column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `dropna()` method on the dataframe. We use the argument `subset`, which you set to `'selftext'`). Check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) if you want a reminder of how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset='selftext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: How many posts are left in the dataset? How many did we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this cleaned-up dataframe in a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/aita_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-Vy6u-1ZWy9"
   },
   "source": [
    "<a id='preprocess'></a>\n",
    "\n",
    "# Preprocessing Text Data with SpaCy\n",
    "\n",
    "Text data collected in the real world is always going to be variable, which poses a challenge for analysis. But by reducing some of this variation, we can help improve our results. For example, if we are counting instances of the word `\"weather\"` in text, we might want the strings `\"weather\"`, `\"weather.\"`, and `\"Weather\"` to all be counted as instances of the same word. However, in raw text form, these would be treated as separate strings. By performing text cleaning, we can standardize these cases and make our data easier to analyze. Some common preprocessing steps are:\n",
    "\n",
    "- Removing punctuation\n",
    "- Removing URLs\n",
    "- Removing stopwords (non-content words like \"a\", \"the\", \"is\", etc.)\n",
    "- Lowercasing\n",
    "- Tokenization (e.g., splitting a sentence into distinct \"chunks\" or \"tokens\")\n",
    "- Stemming, or removing the ends of words (e.g., places -> place)\n",
    "- Lemmatization, or changing words to 'dictionary form' (e.g., runs, running, run -> run)\n",
    "\n",
    "Fortunately, we don't need to code every one of these steps. Instead, we will use a package called [spaCy](https://spacy.io/) to do these things. If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), `spaCy` is ready to use out-of-the-box. We will use the [`en_core_web_sm`](https://spacy.io/models/en/#en_core_web_sm) pipeline to cover the steps listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1405,
     "status": "ok",
     "timestamp": 1647462821529,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "XYssvJqaKnua",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My girlfriend recently went to the beach with a few of her friends.  She has this tiny bikini bottom that is basically a thong that I HATE when she wears in public.  Well she wore it.  Not only did she wear it, she posed in the bathroom mirror of her hotel room to take a side profile picture so you could see her ass sticking out in it and posted it to her Snapchat story.   Worth mentioning I am not friends with her on Snapchat for reasons similar to this (sick of getting in fights when she says she's going out for 'girls' night then posts videos of her sitting at a table with like 5 dudes that always got invited by one of the other girls which was completely unknown to her until she arrived - most of these guys she then adds on Snapchat afterwards).  She didn't even save it and send it to me.  I saw it when she was showing me pics from her beach trip and she had screenshot that particular snap and left it in her camera roll.  Whether the ass part was intentional or not I will never know.  She claims she just liked the way her stomach looked and it was just a pic because she was at the beach and that was the only black bikini bottom she owns.  But there's nothing to look at but her in a micro bikini in a bathroom mirror.  There's no ocean, no sand, no friends, just her tits and ass in a bathroom mirror.  She's 24 and I am 30.  Is it the age?  Is this typical 24 year old behavior nowadays?  Am I wrong for thinking this is inappropriate behavior when you're in a relationship or am I the asshole for making a big deal about it?\n",
      "\n",
      "\n",
      "**Edit:**  Guys, let me clear.  I did not try to control her at any point in time.  I have never once told her what to do, what to wear, what she can or can not do.  Yes, I hate that her ass is completely out in that bikini.  But I've never told her not to wear it and my problem is not that she wore it in general.  She wore it all summer long despite my groaning.  That was never my issue.  I found it super annoying yes but my issue is that she sent out a picture of her ass on Snapchat.  Plain and simple.  That's where I drew the line.  I didn't break it off because she wore it, I broke it off because she turned sideways in a bathroom mirror stuck her butt out, took a picture and sent it out on Snapchat.  It's a borderline nude in that suit and seems wildly inappropriate to me.  Also for clarity I am not jealous.  The friends she went to the beach with were guy friends.  I was completely ok with it.  I had never even met them before.   She has guy friends.  She hangs out with them.  Never my issue and I didn't even feel it was worth mentioning.  My issue with the whole girls night comes into play when I get explicitly told I cannot come because it is girls only night, then later find out it was just a bunch of dudes and her and her girlfriend.  That's not a trust thing and I didn't try to make a big deal about it I just removed her from my friends on Snapchat so I didn't have to see it anymore.  And in all honesty my only true problem with it was one guy that was there in particular.  He's been hitting on her and messaging her non stop since we started dating knowing she has a boyfriend.  Both times I got snubbed for 'girls night', this one particular fella was in attendance both nights and I got the \"I didn't know he was going to be there\" from her.  But again whatever she can do whatever she wants whenever she wants and I've never tried to stop her but for my own sanity I removed her from Snapchat so I didn't have to see it.  That wasn't the point in this post and that wasn't even why I dumped her so I didn't really want to get into detail on that but a few people are calling me controlling and jealous now.  I broke it off because she sent out a pic of her ass on Snapchat.  The only reason we were together is because SHE told me the wild party phase was done and she was looking to start a family and have a serious relationship.  She is a single mom of an almost 3 year old daughter who I love to pieces.  She has been chasing me for 2 years trying to date me and I told her no every step of the way because she was just a little to wild for what I'm looking for in the stage I'm at in life.  So when she came to me earlier this year saying she had matured, all the partying and wild stuff was done and she was ready to get serious and start a family with me, I agreed to commit.  I did not date this woman and say \"HEY PUT ON SOME PANTS\".  I simply said \"you told me this stuff was over and I told you from the very beginning I had no interest in dating a girl who's still stuck in the wild college party phase\" so I ended it.  If she hadn't told me she had matured and wanted to get serious I never would have agreed to date her to begin with.  That's just not what I'm looking for.  I don't think that makes me jealous or controlling.  I was very open and honest about what I was looking for the 2 years I've known her.  \n"
     ]
    }
   ],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Load the English preprocessing pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Parse the first reddit post in the dataset\n",
    "parsed_post = nlp(df.selftext[0])\n",
    "print(parsed_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n94uTq2ZK3X-"
   },
   "source": [
    "The base text looks the same, but we can take a closer look at `parsed_post` to see what happened under the hood. A lot of the information SpaCy has gathered operates on each **token** in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iter'></a>\n",
    "\n",
    "# For-Loops\n",
    "\n",
    "The strength of using computers is their speed. We can leverage this through repeated computation. In Python, we can do this using **loops**. \n",
    "\n",
    "A **[for loop](https://www.w3schools.com/python/python_for_loops.asp)** executes some statements once *for* each value in an iterable (like a list or a string). It says: \"*for* each thing in this group, *do* these operations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA\n",
      "SCIENCE\n",
      "FOR\n",
      "SOCIAL\n",
      "JUSTICE\n"
     ]
    }
   ],
   "source": [
    "tokens = ['Data','science','for','social','justice']\n",
    "\n",
    "for token in tokens:\n",
    "    up = token.upper()\n",
    "    print(up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the syntax of this `for` loop a bit more closely:\n",
    "\n",
    "<img src=\"../../images/for.svg\" alt=\"For loop in Python\" width=\"700\"/>\n",
    "\n",
    "Pay attention to the **loop variable** (`token`). It stands for each item in the list (`tokens`) we are iterating through. Loop variables can have any name; if we'd change it to `x`, it would still work. However, loop variables only exist inside the loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's loop over our `parsed_post` and get some **attributes** that SpaCy has extracted for us. We will save them in a list of dictionary items, then put that list into a DataFrame like we did in Python Fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store token attributes\n",
    "token_data = []\n",
    "\n",
    "# Iterate over tokens and extract attributes\n",
    "for token in parsed_post:\n",
    "    token_data.append({\n",
    "        \"text\": token.text,\n",
    "        \"lemma\": token.lemma_,\n",
    "        \"pos\": token.pos_,\n",
    "        \"tag\": token.tag_,\n",
    "        \"dep\": token.dep_,\n",
    "        \"shape\": token.shape_,\n",
    "        \"is_alpha\": token.is_alpha,\n",
    "        \"is_stop\": token.is_stop,\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the token data\n",
    "token_df = pd.DataFrame(token_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynLOmKULRtwz"
   },
   "source": [
    "As you can see, `spaCy` does a *lot* of work. Let's have a look at the [documentation](https://spacy.io/api/attributes) to see which attributes we are looking at here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA0wy-ttNE7u"
   },
   "source": [
    "## Preprocessing all data\n",
    "Now we can scale up this process to our whole dataset. Up until this point, we have run the preprocessing pipeline on a single post, but we want to automate our code to process all posts at once. \n",
    "\n",
    "To do this, Let's define a few helper functions that we'll use for text normalization. In particular, the `lemmatized_sentence_corpus` generator function will use `spaCy` to:\n",
    "\n",
    "- Iterate over the dataframe.\n",
    "- Segment the threads into individual sentences.\n",
    "- Remove punctuation and excess whitespace.\n",
    "- Lemmatize the text.\n",
    "\n",
    "These helper functions will automate the preprocessing for the posts in this dataset. Don't worry too much about deciphering each line of code. The main goal of these helper functions is to do a lot of the preprocessing for you so that you can use the text in analysis going forward. \n",
    "\n",
    "**You can find this function in the `1_Preprocessing_Project.ipynb` notebook to use on your own data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1647460151917,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qtrgcSZH4Rnb"
   },
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='selftext'):\n",
    "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
    "    for text in df[text_col]:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens\n",
    "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "                  else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"‚Äôs\", \"‚Äô\"] and not lemma in allowed_postags]\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `preprocess()` over our dataframe and look at the first output. Although it looks less like coherent text, it is cleaner and will be much easier to apply natural language processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OjbgskrlDg16"
   },
   "outputs": [],
   "source": [
    "# This may take a while\n",
    "lemmas = [line for line in preprocess(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1639843861051,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "e8fBm9qk-tEb",
    "outputId": "018f2ee6-5596-4715-8a3a-84912495f07d"
   },
   "outputs": [],
   "source": [
    "lemmas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNQTjb50E1Fm"
   },
   "source": [
    "<a id='gensim'></a>\n",
    "\n",
    "# Phrase Modeling with Gensim\n",
    "\n",
    "Many kinds of NLP methods work better when using **N-grams**. An n-gram treats small groups of words as tokens rather than single words. This allows words that frequently appearing together to be concatenated (e.g. \"new york\" means something different and more specific than \"new\" and \"york\" separately). We most commonly use **bigrams** (2-word phrases) and **trigrams** (3-word phrases).\n",
    "\n",
    "**Phrase modeling** is an approach to learning combinations of tokens that together represent meaningful multi-word concepts. So rather than treating every pair of words as a n-gram, we look for pairs that occur together frequently and identify those as n-grams. This constrains the token space by limiting the number of multi-word tokens, but requires information about what words co-occur together frequently, which is where **phrase models** come in. We can develop phrase models by looping over the the words in our lemmatized dataset and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. \n",
    "\n",
    "`gensim` is a popular natural language processing package. We will use it in later lessons for topic modeling and word embeddings. It also contains a [`Phrases`](https://radimrehurek.com/gensim/models/phrases.html) model that implements phrase modeling for identifying bigrams, trigrams, quadgrams, etc. `Phrases` detects phrases based on collocation counts. It builds a model of input text that you then can use on other data.\n",
    "\n",
    "`gensim` detects a bigram if a scoring function for two words exceeds a threshold. The two important arguments to `Phrases` are `min_count` and `threshold`. The higher the values of these parameters, the harder it is for words to be combined to bigrams. We can change the value of these parameters to fine-tune our model. Try changing `min count` and `threshold` below. How does that change the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1639840527645,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "B_VI6UYEBirn",
    "outputId": "d38f9936-85b7-4d98-a847-f4da9a438ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['new_york', 'is', 'great'],\n",
       " ['new_york', 'is', 'in', 'the', 'united', 'states'],\n",
       " ['i', 'love', 'to', 'stay', 'in', 'new_york'],\n",
       " ['people', 'visit', 'the', 'united', 'states']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# \"Documents\"\n",
    "docs = ['new york is great',\n",
    "        'new york is in the united states',\n",
    "        'i love to stay in new york',\n",
    "        'people visit the united states']\n",
    "# Rudimentary tokenization\n",
    "tokens = [doc.split(\" \") for doc in docs]\n",
    "# Create bigrams\n",
    "bigram = Phrases(tokens, min_count=2, threshold=3, delimiter='_')\n",
    "# Freeze bigrams and apply to data\n",
    "bigram_phraser = Phraser(bigram)\n",
    "[bigram_phraser[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a bigram and trigram model for our data. Starting from the preprocessed lemmas from the `preprocess()` function above, we can use the `gensim` models to identify bigrams and trigrams in the dataset. Again, the `min_count` and `threshold` arguments can be modified to change the output of the model. \n",
    "\n",
    "In the code below we make a bigram model using the gensim `Phrases` object, and then build a trigram model on top of that bigram model. Finally, we use the lemmas from the preprocessed text to make a trigram model of the data. We are processing the whole dataset in this cell, so it may take a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63667,
     "status": "ok",
     "timestamp": 1639846696683,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "MxeZ7mc9DeMj",
    "outputId": "a478fee9-f5b1-4562-e3a6-860e244e5719"
   },
   "outputs": [],
   "source": [
    "# Create bigram and trigram models\n",
    "bigram = Phrases(lemmas, min_count=10, threshold=100)\n",
    "trigram = Phrases(bigram[lemmas], min_count=10, threshold=50)  \n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = [trigram_phraser[bigram_phraser[doc]] for doc in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'girlfriend recently beach friend tiny bikini basically thong hate wear public wear wear pose bathroom mirror hotel room profile picture stick post snapchat story worth mention friend snapchat reason similar sick fight girl night post video sit table like dude invite girl completely unknown arrive guy add snapchat save send pic beach trip screenshot particular snap leave camera roll ass intentional know claim like way stomach look pic beach black bikini look micro bikini bathroom mirror ocean sand friend tit ass bathroom mirror age typical year old behavior nowadays wrong think inappropriate behavior relationship asshole big_deal it?**edit guy let clear try control point time tell wear yes hate ass completely bikini tell wear problem wear general wear summer long despite groaning issue find super annoying yes issue send picture ass snapchat plain simple draw line break wear break turn sideways bathroom mirror stick butt picture send snapchat borderline nude suit wildly inappropriate clarity jealous friend beach guy friend completely ok meet guy friend hang issue feel worth mention issue girl night come play explicitly tell come girl night later find bunch dude girlfriend trust thing try big_deal remove friend snapchat anymore honesty true problem guy particular hit message non stop start date know boyfriend time snub girl night particular fella attendance night know want want try stop sanity remove snapchat point post dump want detail people control jealous break send pic ass snapchat reason tell wild party phase look start family relationship single mom year old daughter love piece chase year try date tell step way little wild look stage life come early year mature partying wild stuff ready start family agree commit date woman hey pant simply tell stuff tell beginning interest date girl stick wild college party phase end tell mature want agree date begin look think jealous control open honest look year know'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join each into a string\n",
    "trigrams_joined = [' '.join(trigram) for trigram in trigrams]\n",
    "trigrams_joined[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh5peeC6Fi6H"
   },
   "source": [
    "Once our phrase model has been trained on our total dataset, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1639846257823,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "FCR6DWQeFjt8",
    "outputId": "0c7b6e9c-ebe1-400d-e073-7c104aeddd02"
   },
   "outputs": [],
   "source": [
    "trigram_phraser[\"That\", \"was\", \"not\", \"a\", \"big\", \"deal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bigram parser. We can use `.keys()` to identify the bigrams in the dataset. How many bigrams were identified by the parser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1639846804300,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "xO4enOmLeXXi",
    "outputId": "921b6ce4-3390-4f90-ab13-e7bd4e0ce11f"
   },
   "outputs": [],
   "source": [
    "len(bigram_phraser.phrasegrams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first few bigrams identified in the model as well to check if they seem like appropriate bigrams. If not, we can change the parameters of the bigram model to adjust the sensitivity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1639846859895,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "uwySeLM4df97",
    "outputId": "0940755f-397e-48cc-ecdb-3558341cb7ae"
   },
   "outputs": [],
   "source": [
    "list(bigram_phraser.phrasegrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at trigrams\n",
    "[trigram for trigram in list(trigram_phraser.phrasegrams.keys()) if trigram.count('_') == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVgem81TZWzu"
   },
   "source": [
    "<a id='save'></a>\n",
    "\n",
    "# Saving Data\n",
    "\n",
    "Finally, let's save our data.  In Python, **pickling** is the process of converting Python objects into binary format that can be stored or transferred. You can then reconstruct the original object from that binary format.\n",
    "\n",
    "Pickling is useful when you want to save the state of an object so that it can be used later, or when you need to transfer an object between different Python processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../data/aita_submission_trigrams.pickle', 'wb') as f:\n",
    "    # Save (or \"dump\") the object into the file\n",
    "    pickle.dump(trigrams_joined, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using `dump()`, you'll see the data file gets added in the \"data\" folder, which is in the main folder of this repository. \n",
    "\n",
    "üîî **Question**: Can you find the data file using a file browser (either in Jupyter or on your machine)?\n",
    "\n",
    "‚ö†Ô∏è **Warning:** `aita_trigrams.pickle` is a type of file you can't just open with another program, as it's in binary format. You need to use a special pickle method called `load()` to open the pickled file and load the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/aita_submission_trigrams.pickle', 'rb') as f:\n",
    "    # Load the object from the file\n",
    "    aita_submission_trigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* The Pandas `DataFrame` format can be used to save Reddit data.\n",
    "* Before cleaning text data, it is a good idea to drop rows and columns you don't need.\n",
    "* SpaCy can be used to preprocess textual data, including tokenizatio and lemmatization.\n",
    "* Gensim can be used to combine tokens into N-grams.\n",
    "* Python objects can be easily saved into binary format. This is called \"pickling\".\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 1 Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
