{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20504cb7-8e71-4be6-9eee-5686e94d55d1",
   "metadata": {},
   "source": [
    "# Data Science for Social Justice Workshop: Large Language Models\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand some foundational principles in deep learning.\n",
    "* Understand functional components of transformers in large language models.\n",
    "* Understand the concepts of pre-training and fine-tuning for natural language processing.\n",
    "* Engage with large language models to extract normative principles encoded in the models.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí≠ **Reflection:** Reflecting on ethical implications, biases, and social impact in data science.\n",
    "\n",
    "### Sections\n",
    "1. [Large Language Models and Deep Learning](#intro)\n",
    "2. [A Whirlwind Tour of Deep Learning, Transformers, and Large Language Models](#llms)\n",
    "4. [Chatbots as Reflections of Normative Principles](#chatbots)\n",
    "4. [Reflection and Further Experiments](#reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfd1f6-b977-4fdf-8b59-6f105d018d85",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Large Language Models and Deep Learning\n",
    "\n",
    "Many of the impressive tools that have recently captured public attention - Midjourney images, ChatGPT, etc. - are tools founded in the principles of **deep learning**. Chatbots in particular are a variety of **large language models**, which are **pre-trained** on astronomical amounts of text, and then **fine-tuned** to accomplish specific tasks. The power of these tools is evident, and they will increasingly become integrated into our lives.\n",
    "\n",
    "Our goal in this lesson is not to walk through all the mathematical or architectural details of these models, but to get you up to speed on enough of the core concepts that you can engage with these tools in an informed fashion.\n",
    "\n",
    "Ultimately, we want to use these tools as a way to interrogate the norms they encode. We will be returning to qualitative analysis to better interrogate the norms reinforced by chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489cc164-303d-469e-84dd-bcf4d96c9eeb",
   "metadata": {},
   "source": [
    "<a id='llms'></a>\n",
    "\n",
    "## A Whirlwind Tour of Deep Learning, Transformers, and Large Language Models\n",
    "\n",
    "Let's get through a bunch of core concepts. We're presenting a _lot_ of material at once, so this doesn't all have to make sense - our goal is to just have you be exposed to these concepts so that they are not unfamiliar to you. You can treat this as a glossary that you can refer back to.\n",
    "\n",
    "* **Machine learning** is a field in which _models_ are developed that learn patterns and make predictions from data, without explicitly defining rules. Typically, machine learning algorithms are _trained_ on large datasets to _learn_ patterns within them, which hopefully _generalizes_ to new data.\n",
    "* **Deep learning** is a subfield of machine learning in which _neural networks_ are used to learn patterns from large datasets. Neural networks consist of many _neurons_ which individually perform simple operations. However, in concert, many billions of neurons in a network can perform increasingly expressive computations.\n",
    "![nn](../../images/nn.png)\n",
    "* Various neural networks in deep learning come with different **architectures**. The architecture can dramatically impact the power of a neural network, especially for different types of data. You may have heard of **feedforward neural networks**, which are among the simplest (the above picture is a feedforward neural network). **Convolutional neural networks** have been very popular and effective in computer vision. **Recurrent neural networks** have been the most commonly used networks in time series forecasting and natural language processing, until recently.\n",
    "* **Word embeddings** models are actually very simple neural networks. Recall that what makes a word embeddings powerful is that the model facilitates interactions between nearby words in a sentence.\n",
    "* In 2017, the **transformer** architecture was published. It was specifically designed with natural language processing in mind. The transformer relies on an operation called **self-attention**, where the network learns to model interactions between _all pairwise words in a sentence_. This is not a particularly novel goal, but the self-attention mechanism encoded this concept into the network architecture in a clever way.\n",
    "![transformer](../../images/transformer.png)\n",
    "* If you stack enough transformer blocks on top of each other (along with a few other architectural changes), you obtain a **large language model**. _The transformer is the bedrock of practically every AI tool that interacts with natural language, including ChatGPT_.\n",
    "* It's not enough to simply have a massively large language model. **You also need massive amounts of data**. The reason natural language processing is just now experiencing a revolution in deep learning is because the natural language datasets have become astronomical in size. These include **Common Crawl** (a large fraction of the internet), **Book Corpus** (a large fraction of all books ever written), **Wikipedia**, and **WebText2** (another repository of webpages). \n",
    "* A key concept in large language models is **transfer learning**. This involves a two step procedure: a large language model is first **pre-trained** with a certain objective in mind on a large corpus of data. The model is then **fine-tuned** to perform a specific task (e.g., sentiment analysis, or something similar). The idea here is that the pre-training procedure allows the model to learn a good representation of natural language. The fine-tuning step allows the model to become very good at a specific task.\n",
    "* Take **ChatGPT**, for example: it is based off the GPT model, or **Generative Pre-trained Transformers**. GPT is pre-trained to **predict the next token given the history of the text** (this is where \"generative\" comes from). To obtain ChatGPT, it is then fine-tuned to specifically provide good responses to user queries in a chatbot fashion. Annotator feedback is critical for this fine-tuning procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf2cfa-73c9-4b11-b54a-5cef5036478f",
   "metadata": {},
   "source": [
    "<a id='chatbots'></a>\n",
    "\n",
    "## Chatbots as Reflections of Normative Principles\n",
    "\n",
    "By now, you have likely used ChatGPT, or other tools, to help you with some kind of coding, knowledge, or writing task, including:\n",
    "\n",
    "- Writing an email\n",
    "- Answering a simple question\n",
    "- Trying to understand a complicated topic\n",
    "- Using it to massage, shorten, or edit you writing\n",
    "- Using it to debug or write code\n",
    "\n",
    "and a variety of other tasks. \n",
    "\n",
    "The capacity of these chatbots is a testament to how much deep learning has developed in the past decade. However, it's important that we reflect on them as *products*, and not just machine learning algorithms. We've seen in this workshop that once an algorithm is deployed as a tool, it has real impacts on real people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c457b-5af4-4862-9601-2e4b141340d0",
   "metadata": {},
   "source": [
    "### üí≠ Reflection \n",
    "\n",
    "What are some impacts that these chatbots will have on society, as they are increasingly adopted and woven into our lives. Will there be unintended consequences? \n",
    "\n",
    "There are some obvious cases where chatbots can be actively harmful: misinformation, hate speech, and facilitating plagiarism. Companies like OpenAI already know this and actively are putting resources to addressing them, but it remains to be seen whether these issues can sufficiently addressed.\n",
    "\n",
    "However, think more deeply: how will you use these chatbots? How might they impact your beliefs and decision making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06692351-134c-4d0a-a719-d22d9ec6e3b4",
   "metadata": {},
   "source": [
    "### Probing Chatbots for Norms\n",
    "\n",
    "We've used AITA as an example subreddit because it's a great place to directly interrogate the **norms** of a community. Redditors deem people assholes, or not, and explain why. Thus, norms about what constitutes an \"asshole\" are often directly stated, rather than having to be inferred.\n",
    "\n",
    "Chatbots like ChatGPT make decisions about what information is presented to us, and how it's presented. This by itself is a normative decision. For example, let's ask ChatGPT what the top five qualities of an \"asshole\" are (see the saved chat [here](https://chat.openai.com/share/8e44977c-a2a9-4190-9ccb-e9921ea214dd)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88edd8e-78fa-4ac8-9d80-c4253930481c",
   "metadata": {},
   "source": [
    "<img src=\"../../images/chatgpt1.png\"  width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603660e-a474-46e3-8ff9-1e45dca240fe",
   "metadata": {},
   "source": [
    "ChatGPT (and other chatbots) often have \"hedging\" where they insist that the provided response is not applicable in every scenario (\"It is important to note that these qualities do not define...\").\n",
    "\n",
    "However, at the end of the day, ChatGPT has defined five components of being an asshole. This was a normative decision. If we, as humans, use the outputs of ChatGPT in our day-to-day lives, the norms encoded by ChatGPT will influence our own norms. We need to have systems in place to interrogate and understand these encoded norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623883f4-fa47-4c84-bfc8-11a0e53a0edd",
   "metadata": {},
   "source": [
    "### Prompt Engineering to Elicit Norms\n",
    "\n",
    "ChatGPT's exhibits great flexibility \"prompt engineering\". You can often be creative with the prompt engineering. For example, consider the following examples:\n",
    "\n",
    "1. [Prompting ChatGPT to act as a user of the subreddit](https://chat.openai.com/share/10dddc6a-e32a-4853-9b96-45c0c6a959a4). This uses the system message: _Pretend you are a user of the subreddit \"Am I the Asshole\"_\n",
    "2. [Prompting ChatGPT to evaluate a post on the subreddit](https://chat.openai.com/share/24d71e96-63ed-418d-bcdd-b7bbd5b11430). Compare this to the responses on the actual [post](https://www.reddit.com/r/AmItheAsshole/comments/13x4ppj/aita_for_refusing_to_make_my_son_cut_his_hair_to/)\n",
    "3. [Prompting ChatGPT to act with a different persona](https://chat.openai.com/share/48b4d3f0-9b2c-4b0e-91e6-4722c0ae2e6c). In this example, we ask ChatGPT to act as a \"moral philosopher\". Notice how the character of the response changes.\n",
    "4. [Make full use of the system message](https://chat.openai.com/share/7c99d977-c9ae-42e8-9f1a-0f4ce5e13e66). A common format you can use to prompt ChatGPT is the following:\n",
    "\n",
    "```\n",
    "[System message, telling ChatGPT what persona to take, and what the overall task is].\n",
    "\n",
    "[The details of specific to *this* task, e.g., a Reddit post]\n",
    "\n",
    "[Additional constraints: length, format, and components of answer]\n",
    "```\n",
    "\n",
    "In the last example, we ask ChatGPT to give a response in a particular format, and restrict the length of the answer. We also make our own normative prescriptions in system message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c589d-7032-4247-8da1-98dad99b0245",
   "metadata": {},
   "source": [
    "<a id='reflection'></a>\n",
    "\n",
    "## üí≠ Reflection \n",
    "\n",
    "How can you use your subreddit to probe the norms encoded by ChatGPT?\n",
    "\n",
    "Consider taking recent posts from your subreddit and asking ChatGPT those questions. You could additionally ask ChatGPT to take on different personas. How do the responses by ChatGPT contrast with the top responses in the subreddit? \n",
    "\n",
    "These are difficult questions to answer, and will likely require a close reading. Answering these questions in a quantitative fashion and at scale are active research problems in the community. In these moments, returning to the qualitative work to better inform quantitative analyses will result in a more robust research pipeline.\n",
    "\n",
    "We are especially interested in any creative ways you find to probe or interrogate ChatGPT! Even if you think something won't work, try it anyway - you might be surprised!\n",
    "\n",
    "**Keep in mind: You want to experiment on recent posts from your subreddit. If the posts are older than 2021, chances are ChatGPT was trained on those examples.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
