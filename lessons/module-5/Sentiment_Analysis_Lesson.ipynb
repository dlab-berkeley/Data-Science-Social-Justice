{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Sentiment Analysis\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand the goal of sentiment analysis.\n",
    "* Use VADER to conduct sentiment analyses.\n",
    "* Think critically on how sentiment analysis can be used to interrogate a dataset.\n",
    "* Critique sentiment analysis models, and assess their failure modes.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí≠ **Reflection:** Reflecting on ethical implications, biases, and social impact in data science.\n",
    "\n",
    "### Sections\n",
    "1. [What is Sentiment Analysis?](#sentiment_analysis)\n",
    "2. [Using VADER to Conduct Sentiment Analysis](#vader)\n",
    "3. [Sentiment Analysis on Reddit Data](#vader_reddit)\n",
    "4. [Using Sentiment Analysis to Interrogate Your Data](#analysis)\n",
    "5. [Demo: Sentiment Analysis with spaCy and TextBlob](#demo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "<a id='sentiment_analysis'></a>\n",
    "\n",
    "# What is Sentiment Analysis?\n",
    "\n",
    "Language can convey nuanced ideas, feelings, and emotions. As humans, we're very good at extracting the underlying emotional state expressed by a text. We do this all the time, including:\n",
    "\n",
    "- Figuring out if someone liked or disliked a product in a review,\n",
    "- Understanding whether a colleague is upset or not in an email,\n",
    "- Reading an editorial and discerning an author's position on a topic.\n",
    "\n",
    "Can we computationally make these assessments? **Sentiment Analysis** is a task where we aim to characterize the underlying emotional state expressed by a given text.\n",
    "\n",
    "Let's consider a toy example:\n",
    "\n",
    "```\n",
    "\"This was the best service! Would love to come again!\"\n",
    "```\n",
    "\n",
    "üîî **Question**: This is very clearly expressing **positive sentiment**. How did we make that judgement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Sentiment Analysis on Reddit: A Close Reading\n",
    "\n",
    "On a subreddit like AITA, the manner in which the OP expresses sentiment on the involved parties influences how commenters interpret and ultimately vote on the situation. Expressions of sentiment reflect the norms carried by the community under study.\n",
    "\n",
    "Let's take a look at some example comments. First, we import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/aita_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: Take a look at the following submissions.\n",
    "\n",
    "- What sentiments are expressed by each post?\n",
    "- Can you distill that sentiment down to an \"overall sentiment\" for that post?\n",
    "- Do posts fall neatly within the scope of \"positive\" or \"negative\" sentiment?\n",
    "- What nuances do you lose by choosing to represent the text in this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['selftext'].iloc[463])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['selftext'].iloc[191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['selftext'].iloc[687])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader'></a>\n",
    "\n",
    "# Using VADER to Conduct Sentiment Analysis\n",
    "\n",
    "There are several ways to approach sentiment analysis using natural language processing tools. One of the most commonly used tools is called **VADER** (Valence Aware Dictionary and sEntiment Reasoner).\n",
    "\n",
    "As far as sentiment analyzers go, VADER is pretty simple: it has a dictionary of many words and their associated sentiment (e.g., \"bad\" is negative sentiment, \"happy\" is positive sentiment) and combines these sentiments into \"scores\".\n",
    "\n",
    "Let's import VADER and see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VADER if necessary\n",
    "%pip install vadersentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the package `vadersentiment`, we import something called the `SentimentIntensityAnalyzer`.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** `SentimentIntensityAnalyzer` is not a function we can run to calculate sentiment. It's similar to `TFIDFVectorizer`: we first **instantiate** it into an **object**, and then use that object to analyze the text.\n",
    "\n",
    "Let's create an object called `analyzer` using `SentimentIntensityAnalyzer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VADER SentimentIntensityAnalyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# Create analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use a **method** called `polarity_scores` to calculate sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(\"This was the best service! Would love to come again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER separately provides measures of **positive**, **neutral**, and **negative** sentiment. VADER also provides a `compound` score which synthesizes all three into a single measure of sentiment.\n",
    "\n",
    "üîî **Question**: Do the numbers here match your assessment of the sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: VADER provides [recommendations](https://vadersentiment.readthedocs.io/en/latest/pages/about_the_scoring.html) on how to use the `compound` score as a single measure of sentiment:\n",
    "\n",
    "1. Positive sentiment: `compound` $\\geq 0.5$\n",
    "2. Neutral sentiment: $-0.5 <$ `compound` $<0.5$\n",
    "3. Negative sentiment: `compound` $\\leq -0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** No sentiment analyzer is perfect. Try different examples in VADER to see where it might make errors. How does it handle sarcasm, or convoluted text?\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(\"I am not happy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty accurate. What about this sentence, which says the same thing, but in a more confusing way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(\"It is not the case that I am happy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway here is not that we have to abandon VADER as a tool. Like all computational tools, it has its limits. We need to be aware of these limits, especially when the usage of these tools impacts our beliefs and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader_reddit'></a>\n",
    "\n",
    "# Sentiment Analysis on Reddit Data\n",
    "\n",
    "Let's try applying the sentiment analyzer to the Reddit data. First, let's apply it to the three posts we considered above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analyzer.polarity_scores(df['selftext'].iloc[463]))\n",
    "print(analyzer.polarity_scores(df['selftext'].iloc[191]))\n",
    "print(analyzer.polarity_scores(df['selftext'].iloc[687]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: Do you agree with how VADER characterized these post? \n",
    "\n",
    "Ultimately, like most natural language processing tasks, there is no real **ground truth**: the answer is subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterizing Sentiment at Scale\n",
    "\n",
    "We've looked at posts one by one, but what if we want to calculate sentiment on our entire dataset?\n",
    "\n",
    "‚ö†Ô∏è **Warning:** If we pass in the entire list of posts to `polarity_scores`, VADER will calculate the sentiment on the **whole** dataset. We have to use the function one post at a time.\n",
    "\n",
    "We will need to use a **for loop** to perform this repeated computation. Let's think through how to do this:\n",
    "\n",
    "- We iterate through every selftext in the dataset\n",
    "- For each selftext, we calculate each sentiment. We'll focus on the compound score, for simplicity.\n",
    "- We store the score in a data structure. Let's use a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store scores\n",
    "compound_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few minutes to run\n",
    "\n",
    "# Iterate through the selftext of each post\n",
    "for post in df['selftext']:\n",
    "    # Calculate sentiment\n",
    "    sentiment = analyzer.polarity_scores(post)\n",
    "    # Store each score\n",
    "    compound_scores.append(sentiment['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the compound scores in the dataframe\n",
    "df['sentiment'] = compound_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of sentiment. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].hist(grid=False)\n",
    "plt.xlabel('Compound Sentiment Score', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "\n",
    "# Using Sentiment Analysis to Interrogate Your Data\n",
    "\n",
    "In the literature, most papers on sentiment analysis are concerned with the task itself: Can we build a better model to conduct sentiment analysis? How good is the model? Comparatively less papers actually leverage sentiment analysis to understand norms about society. \n",
    "\n",
    "üí≠ **Reflection:** Why is there so much focus on improving models, and less usage of these models to critically examine corpora? What value structures are in place to incentivize this kind of research?\n",
    "\n",
    "Let's use sentiment analysis to try and better understand this dataset and community. To do so, we need to have some research questions. These will motivate analyses, the results of which will motivate further research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are There Differences in Sentiment Between \"YTA\" Posts and \"NTA\" Posts?\n",
    "\n",
    "What if we looked at sentiment separately for \"YTA\" flaired posts and \"NTA\" flaired posts?\n",
    "\n",
    "Remember, we are examining sentiment of posts written by original authors *as a function of their behavior being rated by community members*. So, the manner in which they express sentiment could correlate with being rated \"YTA\" or \"NTA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter posts by YTA / NTA text\n",
    "yta_posts = df[df['flair_text'] == 'Asshole']\n",
    "nta_posts = df[df['flair_text'] == 'Not the A-hole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own axes to plot on\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# Plot the histogram of sentiment for YTA\n",
    "yta_posts.hist('sentiment',\n",
    "               # Use a histogram type where it shows only the border\n",
    "               histtype='step',\n",
    "               # Set the line width of the histogram\n",
    "               lw=2,\n",
    "               # Turn the grid off\n",
    "               grid=False,\n",
    "               # \"Density\" allows us to compare multiple histograms, even if the total counts are different\n",
    "               density=True,\n",
    "               # Set the label for the legend\n",
    "               label='YTA',\n",
    "               # Be sure to use the axes we create\n",
    "               ax=ax)\n",
    "\n",
    "# Plot the histogram of sentiment for NTA\n",
    "nta_posts.hist('sentiment',\n",
    "               # Use a histogram type where it shows only the border\n",
    "               histtype='step',\n",
    "               # Set the line width of the histogram\n",
    "               lw=2,\n",
    "               # Turn the grid off\n",
    "               grid=False,\n",
    "               # \"Density\" allows us to compare multiple histograms, even if the total counts are different\n",
    "               density=True,\n",
    "               # Set the label for the legend\n",
    "               label='NTA',\n",
    "               # Be sure to use the axes we create\n",
    "               ax=ax)\n",
    "# Create legend\n",
    "plt.legend(loc='upper center', prop={'size': 15})\n",
    "# Create labels\n",
    "plt.xlabel('Compound Sentiment Score', fontsize=15)\n",
    "plt.ylabel('Density', fontsize=15)\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions look pretty similar, but we see that NTA posts generally have negative sentiment, while YTA posts have a little bit more positive sentiment. The distributions are also very polarizing, with many posts being assigned scores close to -1 or 1, at the extreme.\n",
    "\n",
    "This is pretty interesting, but also a little suspicious. Are there posts that *really* have that extreme positive and negative sentiment? Now is a good point to turn back to a close reading, in order to better elucidate this difference.\n",
    "\n",
    "Let's look at an example YTA post which VADER said had among the highest of positive sentiment:\n",
    "\n",
    "**Content Warning:** Homophobia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the YTA posts by sentiment, in decreasing order, so the most positive posts are on top\n",
    "print(yta_posts.sort_values('sentiment', ascending=False)['selftext'].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some observations from this post?\n",
    "\n",
    "- The original post is pretty clearly not positive sentiment. It probably leans a bit negative in sentiment, especially with how the OP describes the entire situation.\n",
    "- OP's views on their friend's sexuality are reprehensible, but they do praise their friendship, which is likely contributing to the positive sentiment.\n",
    "- There are many edits to this post. This is pretty common in AITA. Many of the edits contain \"thank you\", as well as a paraphrased text to OP's friend that positively characterizes their friendship.\n",
    "\n",
    "These observations demonstrate the complexity in assigning sentiment to text, and demonstrate how close reading allows us to be more critical of the outputs provided by NLP algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are Edits to Posts Inflating Sentiment?\n",
    "\n",
    "In the above example, it is likely that the sentiment analysis algorithm is picking up on these edits when assigning sentiment. That motivates another research question: does the presence of an edit correlate with a certain type of sentiment?\n",
    "\n",
    "Often times, edits involving thanking the community for their response. It is possible that posts with edits may generally have higher sentiment.\n",
    "\n",
    "Let's try and do a cursory analysis to test this. There are many ways we could check for edits, but a pretty simple one is if the post contains the string `'Edit:'`. Let's filter by this criteria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_posts = df[df['selftext'].str.contains('Edit:')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same plot as above, but now we add in the sentiment from the edited posts dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own axes to plot on\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# Plot the histogram of sentiment for YTA\n",
    "yta_posts.hist('sentiment',\n",
    "               # Use a histogram type where it shows only the border\n",
    "               histtype='step',\n",
    "               # Set the line width of the histogram\n",
    "               lw=2,\n",
    "               # Turn the grid off\n",
    "               grid=False,\n",
    "               # \"Density\" allows us to compare multiple histograms, even if the total counts are different\n",
    "               density=True,\n",
    "               # Set the label for the legend\n",
    "               label='YTA',\n",
    "               # Be sure to use the axes we create\n",
    "               ax=ax)\n",
    "\n",
    "# Plot the histogram of sentiment for NTA\n",
    "nta_posts.hist('sentiment',\n",
    "               # Use a histogram type where it shows only the border\n",
    "               histtype='step',\n",
    "               # Set the line width of the histogram\n",
    "               lw=2,\n",
    "               # Turn the grid off\n",
    "               grid=False,\n",
    "               # \"Density\" allows us to compare multiple histograms, even if the total counts are different\n",
    "               density=True,\n",
    "               # Set the label for the legend\n",
    "               label='NTA',\n",
    "               # Be sure to use the axes we create\n",
    "               ax=ax)\n",
    "\n",
    "# Plot the histogram of sentiment for NTA\n",
    "edit_posts.hist('sentiment',\n",
    "                # Use a histogram type where it shows only the border\n",
    "                histtype='step',\n",
    "                # Set the line width of the histogram\n",
    "                lw=2,\n",
    "                # Turn the grid off\n",
    "                grid=False,\n",
    "                # \"Density\" allows us to compare multiple histograms, even if the total counts are different\n",
    "                density=True,\n",
    "                # Set the label for the legend\n",
    "                label='Edited',\n",
    "                # Be sure to use the axes we create\n",
    "                ax=ax)\n",
    "\n",
    "# Create legend\n",
    "plt.legend(loc='upper center', prop={'size': 15})\n",
    "# Create labels\n",
    "plt.xlabel('Compound Sentiment Score', fontsize=15)\n",
    "plt.ylabel('Density', fontsize=15)\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It definitely looks like the distribution has shifted to the right! So, we do see a correspondence between edited posts and sentiment.\n",
    "\n",
    "üîî **Question**: How could we improve this analysis? What other ways do OPs indicate they are updating or editing a post?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Post Score Correlate with Sentiment?\n",
    "\n",
    "Another research question we can ask, which applies to any subreddit, is whether post score correlates with sentiment. We might expect, for example, that posts with very positive or negative sentiment generally get upvoted more, since they express stronger emotions.\n",
    "\n",
    "Let's examine this by plotting the average score of posts with sentiment values in specific ranges. You can use the following function to create this plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "import numpy as np\n",
    "\n",
    "def plot_score_vs_sentiment(sentiment, score, n_bins=9):\n",
    "    \"\"\"Plots the average score within ranges of sentiment values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentiment : pd.Series\n",
    "        The sentiment column from your dataframe.\n",
    "    score : pd.Series\n",
    "        The score column from your dataframe.\n",
    "    n_bins : int\n",
    "        The number of bins to plot.\n",
    "    \"\"\"\n",
    "    # Calculate binned sentiment values\n",
    "    bin_means, bin_edges, binnumber = binned_statistic(sentiment,\n",
    "                                                       score,\n",
    "                                                       statistic='mean',\n",
    "                                                       bins=np.linspace(-1, 1, n_bins))\n",
    "    # Calculate bin width of bar plot\n",
    "    binwidth = np.ediff1d(bin_edges)[0]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.bar(x=bin_edges[:-1] + binwidth / 2, height=bin_means, width=binwidth)\n",
    "    ax.set_xlim([-1, 1])\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_vs_sentiment(df['sentiment'], df['score'], n_bins=10)\n",
    "plt.xlim([-1.05, 1.05])\n",
    "plt.ylim([5000, 6700])\n",
    "plt.xlabel('Compound Sentiment', fontsize=15)\n",
    "plt.ylabel('Average Post Score', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We definitely observe a pattern: Posts with the most negative and most positive sentiment generally have higher scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo1'></a>\n",
    "\n",
    "# üé¨ Demo : Sentiment Analysis with spaCy and TextBlob\n",
    "\n",
    "VADER is not the only tool we can use for sentiment analysis. Within spaCy, which we used for preprocessing, we can use a package called TextBlob to conduct sentiment analysis. This demo walks you through the code to use spaCy to perform sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform these installs first\n",
    "%pip install textblob\n",
    "%pip install spacytextblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NLP object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Important: we have to add textblob to our spaCy pipeline\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the spaCy pipeline to each post\n",
    "# This command will take a while to run if your dataset is big\n",
    "docs = list(nlp.pipe(df['selftext']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob calculates sentiments in the variable \"polarity\". It also includes a variable called \"subjectivity\", which ranges from 0 to 1. It estimates the level of subjectivity expressed in the post (values closer to 1 are higher subjectivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the polarities in a list\n",
    "polarities = []\n",
    "for doc in docs:\n",
    "    polarities.append(doc._.polarity)\n",
    "df['polarities'] = polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the subjectivities in a list\n",
    "subjectivities = []\n",
    "for doc in docs:\n",
    "    subjectivities.append(doc._.subjectivity)\n",
    "df['subjectivity'] = subjectivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new subsetted dataframes\n",
    "yta_posts = df[df['flair_text'] == 'Asshole']\n",
    "nta_posts = df[df['flair_text'] == 'Not the A-hole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.hist(yta_posts['polarities'],\n",
    "        density=True,\n",
    "        histtype='step',\n",
    "        lw=2,\n",
    "        bins=np.linspace(-1, 1, 51),\n",
    "        label='YTA')\n",
    "ax.hist(nta_posts['polarities'],\n",
    "        density=True,\n",
    "        lw=2,\n",
    "        histtype='step',\n",
    "        bins=np.linspace(-1, 1, 51),\n",
    "        label='NTA')\n",
    "ax.legend(prop={'size': 15})\n",
    "plt.xlim([-0.5, 0.5])\n",
    "plt.xlabel('Polarity', fontsize=15)\n",
    "plt.ylabel('Density', fontsize=15)\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.hist(yta_posts['subjectivity'],\n",
    "        density=True,\n",
    "        histtype='step',\n",
    "        lw=2,\n",
    "        bins=np.linspace(0, 1, 51),\n",
    "        label='YTA')\n",
    "ax.hist(nta_posts['subjectivity'],\n",
    "        density=True,\n",
    "        lw=2,\n",
    "        histtype='step',\n",
    "        bins=np.linspace(0, 1, 51),\n",
    "        label='NTA')\n",
    "ax.legend(prop={'size': 15})\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel('Subjectivity', fontsize=15)\n",
    "plt.ylabel('Density', fontsize=15)\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how different the sentiment looks from VADER! We still, however, observe that YTA posts generally have a slightly higher sentiment than NTA posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Sentiment analysis aims to characterize the emotional state of text.\n",
    "* The package VADER can be used to easily calculate sentiment on posts. \n",
    "* Sentiment analyses can facilitate interrogations of the dataset.\n",
    "* Different packages might give generate sentiments.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
