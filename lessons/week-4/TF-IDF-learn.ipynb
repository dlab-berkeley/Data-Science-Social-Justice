{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: TF-IDF\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand the TF-IDF algorithm, and how it's used in information retrieval.\n",
    "* Understand the difference between TF-IDF and simple word counts.\n",
    "* Use Scikit Learn to extract TF-IDF scores from our text data.\n",
    "* Use TF-IDF scores to compare posts.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Term Frequency-Inverse Document Frequency (TF-IDF)](#tfidf)\n",
    "2. [Reading Data with Pandas](#read)\n",
    "3. [Dropping Columns and Missing Values](#drop)\n",
    "4. [Preprocessing Text Data with SpaCy](#clean)\n",
    "5. [Phrase Modeling with Gensim](#gensim)\n",
    "6. [Saving Data](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "<a id='tfidf'></a>\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "In the previous notebook, we covered methods to clean and tokenize text into units (like words, bigrams, and trigrams).\n",
    "\n",
    "For many data science applications, we need a way to convert the content of a text (the list of tokens) into useful numbers that can be used as the features of a model or analysis. One way to do this is to to use a method called Term Frequency-Inverse Document Frequency (TF-IDF). \n",
    "\n",
    "This ultimately is a question of **representation**. A text representation is easier for humans to understand, but a numerical representation is easier to perform large scale computational analysis on. The more we change the representation, the more distant our reading of the text becomes. We may gain some benefits from this, but we must also keep in mind what we lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Retrieving the Dataset\n",
    "\n",
    "We retrieve the data, as we did in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/aita_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22903,
     "status": "ok",
     "timestamp": 1647477678422,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qqV-FJC9aHuT",
    "outputId": "9126a240-03b0-4403-eda9-afb71f673262"
   },
   "outputs": [],
   "source": [
    "with open('../../data/aita_trigrams.pickle', 'rb') as f:\n",
    "    # Load the object from the file\n",
    "    aita_trigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the documents in our cleaned-up `df` is the same length as the list `aita_trigrams` we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) == len(aita_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpWynSoy4t8o"
   },
   "source": [
    "## Implementing TF-IDF\n",
    "TF-IDF, short for **term frequency‚Äìinverse document frequency**, is a metric that reflects how important a word is to a **document** in a collection or **corpus**. When talking about text datasets, the dataset is called a corpus, and each datapoint is a document. A document can be a post, a paragraph, a webpage, whatever is considered the individual unit of text for a given datset. A **term** is each unique token in a document (we previously also referred to this as **type**). \n",
    "\n",
    "For example in a corpus of sentences, a document might be: `\"I went to New York City in New York state.\"` \n",
    "\n",
    "The processed tokens in that document might be: `[went, new_york, city, new_york, state]`.\n",
    "\n",
    "The document would have four unique terms: `[went, new_york, city, state]`.\n",
    "\n",
    "The TF-IDF value increases proportionally to the number of times a word appears in the document (the term frequency, or TF), and is offset by the number of documents in the corpus that contain the word (the inverse document frequency, or IDF). This helps to adjust for the fact that some words appear more frequently in general ‚Äì such as articles and prepositions.\n",
    "\n",
    "We won't go into much detail about the math behind calculating the TF-IDF (see the D-Lab Text Analysis workshop videos to see more). The key components to remember are:\n",
    "\n",
    "1. There is one TF-IDF score per unique word and unique document.\n",
    "2. A high TF-IDF score suggests that word is descriptive of that document.\n",
    "3. A low TF-IDF score may be because either the word is not frequent in that document, or that it is frequent in many documents in the dataset - either way, it may not be a good descriptor of that document.\n",
    "\n",
    "The intuition is that if a word occurs many times in one post but rarely in the rest of the corpus, it is probably useful for characterizing that post; conversely, if a word occurs frequently in a post but also occurs frequently in the corpus, it is probably less characteristic of that post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [`CountVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from a package called `scikit-learn` (one of the most commonly used machine learning packages in Python) to generate the term-frequency matrix for a given corpus, which is the first part of calculating the tf-idf.\n",
    "\n",
    "This is a matrix where the rows corresponds to documents columns correspond to terms in the corpus. Each cell gives a count for the frequency of that term in the document (which may be zero). Let's try it on a toy dataset. Conveniently, there is a built-in tokenizer in `CountVectorizer()` that we will use for now to preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1639773021096,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "GLxYUFszEA0M",
    "outputId": "ff40b717-b7f4-4710-bd43-a0c7613422f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "  'My cat has paws.',\n",
    "  'Can we let the dog out?',\n",
    "  'Our dog really likes the cat but the cat does not agree.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# Use this if your scikit-learn is older\n",
    "# pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtaTTaP8EA0M"
   },
   "source": [
    "Each column in the matrix represents a unique word in the vocabulary, while each row represents the document in our dataset. In this case, we have three sentences (i.e. the document), and therefore we three rows. The values in each cell are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsNok5KyEA0M"
   },
   "source": [
    "Lets look at the shape of the matrix. How many unique words are there in the matrix?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1639773036420,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6mKvaLCnEA0M",
    "outputId": "259f70fb-d186-4330-d9a7-d726459105fa"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kPKfAnFvKY"
   },
   "source": [
    "Now, we have numbers representing the contents of the documents! Matrices like this are the simplest way to represent texts. However, it biases most frequent words and ends up ignoring rare words which could have helped is in processing our data more efficiently.\n",
    "\n",
    "Often, we not only want to focus on the frequency of words present in the corpus but also want to know the importance of the words. This is where TF-IDF (term frequency-inverse document frequency) comes in. TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by combining the **term frequency** calculated above and the **inverse document frequency** of the word across a set of documents. For the latter, we divide the total number of documents by the number of documents containing the term, and then take the logarithm of that quotient. (This is calculated for us by the computer). This tells us if a word is common or rare across all documents.\n",
    "\n",
    "The combination of these two metrics into the TF-IDF will identify how unique a term is to that document.\n",
    "\n",
    "**Note**: Word order is not retained in this type of featurization, since all that is counted is overall frequency of a word in a document. This is called a **bag-of-words** approach, and significantly simplifies the representation problem, but has been found to be effective in capturing key features of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxZ-5f6ZFWe"
   },
   "source": [
    "### Testing TF-IDF with a toy dataset\n",
    "\n",
    "Let's try TF-IDF out with a toy dataset. Here we have three documents about Python, but with different meanings. If we are trying to distinguish between these documents, the word \"Python\" would not be very useful, since it occurs in all of the documents, but other terms might, like \"Monty\", \"snake\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjjWn-phZFWf"
   },
   "outputs": [],
   "source": [
    "document1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Python, from the Greek word (œÄœçŒ∏œâŒΩ/œÄœçŒ∏œâŒΩŒ±œÇ), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Monty Python (also collectively known as the Pythons) are a British \n",
    "surreal comedy group who created the sketch comedy television show Monty Python's \n",
    "Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made \n",
    "over four series.\"\"\"\n",
    "\n",
    "document4 = \"\"\"Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes \n",
    "code readability with its notable use of significant whitespace. Its language constructs and \n",
    "object-oriented approach aim to help programmers write clear, logical code for small and \n",
    "large-scale projects.\"\"\"\n",
    "\n",
    "document5 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\". It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment.\"\"\"\n",
    "\n",
    "document6 = \"\"\"The Pythonidae, commonly known simply as pythons, from the Greek word python \n",
    "(œÄœÖŒ∏œâŒΩ), are a family of nonvenomous snakes found in Africa, Asia, and Australia. \n",
    "Among its members are some of the largest snakes in the world. Eight genera and 31\n",
    "species are currently recognized.\"\"\"\n",
    "\n",
    "test_list = [document1, document2, document3, document4, document5, document6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESR7eMThTkZE"
   },
   "source": [
    "Let's `CountVectorizer()` again, and customize some parameters. Using `max_df` (max document frequency) we can get rid of words that appear in more than 85% of the corpus, and using `stop_words` we can insert an English stopword list to leave out of the calculations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776660955,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Os8jrh3RM_GJ",
    "outputId": "b7adadfc-b6fa-4752-b8c1-08527bd07703"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85, stop_words='english')\n",
    "word_count_vector = cv.fit_transform(test_list)\n",
    "pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names_out())\n",
    "# Use this if your scikit-learn is older\n",
    "# pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkpVqpPzNHRd"
   },
   "source": [
    "How many documents and unique words are in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLHjD29ZFWi"
   },
   "source": [
    "## Using `TfidfTransformer`\n",
    "\n",
    "Next, we need to compute the inverse document frequency values. We'll call [`tfidf_transformer.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) on the word counts we computed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776806708,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "WeTAynM9xxxr",
    "outputId": "a5c7fd51-b4cf-4c16-e643-388303891e01"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer() \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uoj5xOi8vu_"
   },
   "source": [
    "To get a glimpse of how the IDF values look, let's put these into a DataFrame and sort by weights. Remember, a low IDF indicates something that is less unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1639776807884,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "gRjP38dV-7I7",
    "outputId": "ba06ac97-0405-47f1-d01d-db1bbf5ca801"
   },
   "outputs": [],
   "source": [
    "# Print IDF values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=[\"idf_weights\"]) \n",
    "# Sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFeXhx71O_4n"
   },
   "source": [
    "Notice that the words \"python\" and \"in\" have the lowest IDF values. This is expected: these words appear in each and every document in our collection. The lower the IDF value of a word, the less unique it is to any particular document.\n",
    "\n",
    "Now that we have the idf values, we can compute the TF-IDF scores for our set of documents using `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWPqOTR_AIM0"
   },
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0-6H9L9PnLA"
   },
   "source": [
    "By invoking `tfidf_transformer.transform()` we are computing the TF-IDF scores for our docs. Internally, this is weighting TF scores by their IDF scores, so that the more unique a word, the more its frequency counts in a given document.\n",
    "\n",
    "Let‚Äôs print the TF-IDF values of the first document to see if it makes sense. We place the TF-IDF scores from the third document into a `pandas` data frame and sort it in descending order of scores. We can replace the index in `tf_idf_vector[]` to select a different document to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1639776812749,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Qu40IZhpP0b2",
    "outputId": "fbf33f79-7be4-4c0e-8853-02ab6b7cf616"
   },
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names_out() \n",
    "  \n",
    "# Print the scores \n",
    "df = pd.DataFrame(tf_idf_vector[2].T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9qVYEm_ZFW2"
   },
   "source": [
    "What are the most distinctive words for document 3 (Highest TF-IDF scores?) Does it made sense given the document and corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq3QvXEcZFXS"
   },
   "source": [
    "## Using TF-IDF on Reddit datasets\n",
    "\n",
    "Now that we have a good grasp of how TF-IDF is calculated, let's perform this method on our Reddit data. This time, to save time, we will be using `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer). It is a class that basically allows us to create a matrix of word counts (what we just did with `CountVectorizer`), and immediately transform them into TF-IDF values.\n",
    "\n",
    "We simply instantiate an object of the `TfidfVectorizer`. Then, we run it by applying the `fit_transform()` method to our two-document corpus `aita_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vRKbLpDZFXY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                   decode_error='ignore',\n",
    "                                   stop_words='english',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf = tfidf_vectorizer.fit_transform(aita_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the TF-IDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1639778604305,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cghW5hxxZ8Ec",
    "outputId": "d6b9cf01-37c7-4fc1-dc6d-3c8aff1a6e16"
   },
   "outputs": [],
   "source": [
    "# Place TF-IDF values in a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest TF-IDF values across documents\n",
    "tfidf_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YoFv8pXZFXa"
   },
   "source": [
    "## Using TF-IDF to find Similar Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjvztHrBZFXb"
   },
   "source": [
    "We can also use TF-IDF to work out the similarity between any pair of documents. So given one post or comment, we could see which posts or comments are most similar. This can be useful if you're trying to find other examples of a pattern you have found and want to explore further.\n",
    "\n",
    "This time, our \"documents\" will not be entire subreddits, but posts/submissions within one subreddit. Let's import the submissions and run the vectorizer without the preprocessing and lemmatizing. Tf-idf will still work this way, and this way, we will be able to read our posts.\n",
    "\n",
    "We'll use the TF-IDF vectorized output from the previous section. Let's choose a particular document, and try and find similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'].iloc[doc_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6l6lqgF4B0L"
   },
   "source": [
    "Let's have a quick look at the TF-IDF scores for the words in this submission to see if these words are indeed typical for this particular submission. Do the distinctive words have to do with the topic of the post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639779140786,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rpOde4Sr4AzJ",
    "outputId": "8beb14d4-9dab-4266-8f69-4a9ca73259cc"
   },
   "outputs": [],
   "source": [
    "tfidf_df.loc[doc_idx].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiTzCg4xZFXi"
   },
   "source": [
    "Now let's find the closest posts to this one. The fact that our documents are now in a vector space allows us to make use of mathematical similarity metrics.\n",
    "\n",
    "**Cosine similarity** is one metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is equal to 1 if the documents are the same, and decreases to 0 the more dissimilar they are.\n",
    "\n",
    "We can use a cosine similarity function from `sklearn` to calculate the cosine similarity between each pair of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rbfy9VoCEiPc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(tfidf)\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the text and scores in a dataframe, and sort by the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_df = pd.DataFrame({\n",
    "    'text': df['selftext'].values,\n",
    "    'score': similarities[doc_idx]}).sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top document will be the document itself (it's going to have a similarity of 1 with itself). So we look at the next document - does it seem similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1639779186306,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "-dEbe-PoIizd",
    "outputId": "012bdd5a-c563-4a73-9806-9ee882836949"
   },
   "outputs": [],
   "source": [
    "similar_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_df['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Term Frequency, or TF, reflects how often a unique token appears in a corpus.\n",
    "* Inverse Document Frequency (IDF) reflects the number of documents in the corpus that contain a term. \n",
    "* The TF-IDF score of a word reflects how important that word is to a document in a collection or corpus.\n",
    "* Methods from `scikit-learn` can be used to generate TF and TF-IDF scores for a given corpus.\n",
    "* TF-IDF scores can be used to calculate how similar documents are. We can do this with mathematical similarity metrics, such as cosine similarity.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
