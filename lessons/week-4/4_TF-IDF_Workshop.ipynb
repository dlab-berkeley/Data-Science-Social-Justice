{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: TF-IDF\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand the TF-IDF algorithm, and how it's used in information retrieval.\n",
    "* Understand the difference between TF-IDF and simple word counts.\n",
    "* Use Scikit Learn to extract TF-IDF scores from our text data.\n",
    "* Use TF-IDF scores to compare posts.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí≠ **Reflection**: Reflecting on ethical implications, biases, and social impact in data science.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Turning Words into Numbers](#words)\n",
    "2. [Bag-Of-Words Representation](#bow)\n",
    "3. [Testing TF-IDF with a toy dataset](#toy)\n",
    "4. [Using TF-IDF on Reddit datasets](#reddit)\n",
    "5. [Using TF-IDF to Find Similar Posts](#similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "<a id='words'></a>\n",
    "\n",
    "# Turning Words Into Numbers\n",
    "\n",
    "In the previous notebook, we covered methods to clean and tokenize text into units (like words, bigrams, and trigrams).\n",
    "\n",
    "For many data science applications, we need a way to convert the content of a text (the list of tokens) into useful numbers that can be used as the features of a model or analysis. One way to do this is to to use a method called Term Frequency-Inverse Document Frequency (TF-IDF). \n",
    "\n",
    "This ultimately is a question of **representation**. A text representation is easier for humans to understand, but a numerical representation is easier to perform large scale computational analysis on. The more we change the representation, the more distant our reading of the text becomes. We may gain some benefits from this, but we must also keep in mind what we lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Retrieving the Dataset\n",
    "\n",
    "We retrieve the data, as we did in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/aita_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22903,
     "status": "ok",
     "timestamp": 1647477678422,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qqV-FJC9aHuT",
    "outputId": "9126a240-03b0-4403-eda9-afb71f673262"
   },
   "outputs": [],
   "source": [
    "with open('../../data/aita_submission_trigrams.pickle', 'rb') as f:\n",
    "    # Load the object from the file\n",
    "    aita_trigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the documents in our cleaned-up `df` is the same length as the list `aita_trigrams` we created in the previous notebook. We can also check whether the last document features the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) == len(aita_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16312</th>\n",
       "      <td>19999</td>\n",
       "      <td>1662311673</td>\n",
       "      <td>t3_rhp3w9</td>\n",
       "      <td>1639655198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Due-Understanding536</td>\n",
       "      <td>AITA for suggesting to my fianc√©e that we make...</td>\n",
       "      <td>I'm getting married in 7 months and one week a...</td>\n",
       "      <td>5366.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1613.0</td>\n",
       "      <td>1761.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       idint      idstr     created  nsfw  \\\n",
       "16312       19999  1662311673  t3_rhp3w9  1639655198   0.0   \n",
       "\n",
       "                     author  \\\n",
       "16312  Due-Understanding536   \n",
       "\n",
       "                                                   title  \\\n",
       "16312  AITA for suggesting to my fianc√©e that we make...   \n",
       "\n",
       "                                                selftext   score distinguish  \\\n",
       "16312  I'm getting married in 7 months and one week a...  5366.0         NaN   \n",
       "\n",
       "       textlen  num_comments flair_text flair_css_class  \n",
       "16312   1613.0        1761.0        NaN             NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marry month week wedding plan honeymoon resort country hour flight place dream destination friend tell plan honeymoon jealous wish tell consider idea friend inclusive honeymoon excited idea sadly year able plan trip guy place conflict schedule pretty easy able align vacation day think great chance announce fianc√©e day think find cool idea completely mad start cry tell inconsiderate ah try explain malice simply think good chance guy experience tell tell friend want join tell ridiculous insist good idea feel horrible inconsiderate honeymoon friendship trip suppose big ah discuss friend excited consult honestly think right ta.on idea think mind think sound fun aita'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita_trigrams[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the `2_TF_IDF_Project.ipynb` notebook to run the TF-IDF operations explained in this notebook on your own data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bow'></a>\n",
    "\n",
    "# Bag-Of-Words Representation\n",
    "\n",
    "Before we do TF-IDF, we need to learn something about **bag-of-words (BOW)** models.\n",
    "\n",
    "The idea of BOW models is to encode the corpus in terms of word frequencies. In a bag-of-words, we taken some text, tokenize it, and then tabulate the frequencies of each token. The numerical representation of the text, then, is a vector indicating the frequencies of each token for that text.\n",
    "\n",
    "For example, if we're considering a Reddit post on r/amitheasshole: \n",
    "\n",
    "<img src=\"../../images/bow.svg\"  width=\"600\" height=\"300\">\n",
    "\n",
    "\n",
    "We take each token from the review, \"toss it in a bag\", and count up the frequencies of each word. The numerical representation, then, is the vector on the right: the number of appearances of each token. The \"bag\" here denotes that we are not modeling structure within the text - only the frequencies of the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix\n",
    "\n",
    "In most text corpora, we will have many samples or *documents*. For example, in our Reddit dataset, we have many posts. Each post stands on its own as a unique sample: it can be thought of as a unique document in the entire *corpus*. \n",
    "\n",
    "Since they are all related to each other, many tokens might be shared across posts. So, when creating the bag-of-words model, we can tokenize across all documents, forming a *vocabulary*. Then, we can represent a single document by which of the tokens in the vocabulary are represented, and their frequency within the document.\n",
    "\n",
    "If the vocabulary has $V$ tokens, then each document will be encoded in a $V$-dimensional vector. If there are $D$ documents, the entire dataset can be represented in a $D \\times V$ matrix, where each row corresponds to the document, and each column corresponds to the token (or \"term\"). This $D \\times V$ matrix is a **document term matrix** (DTM).\n",
    "\n",
    "Let's consider a simple example. Suppose we have the \"documents\":\n",
    "\n",
    "```\n",
    "[\"You are at a workshop. Are you ready?\",\n",
    " \"Welcome to Berkeley!\",\n",
    " \"I am teaching a workshop.\"]\n",
    "```\n",
    "\n",
    "The unique (word) tokens in this \"corpus\", in alphabetical order, are:\n",
    "\n",
    "```\n",
    "[a, am, are, at, berkeley, i, ready, teaching, to, welcome, workshop, you]\n",
    "```\n",
    "\n",
    "The DTM can be formed by going through each document, ticking off the frequency of each token in each document, and plugging this number into the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{a} & \\text{am} & \\text{are} & \\text{at} & \\text{berkeley} & \\text{i} & \\text{ready} & \\text{teaching} & \\text{to} & \\text{welcome} & \\text{workshop} & \\text{you} \\\\\\hline\n",
    "\\text{Document 1} & 1 & 0 & 2 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "\\text{Document 2} & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "\\text{Document 3} & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The numerical representation for each document is a row in the matrix. For example, Document 1 has numerical representation $[1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 1]$.\n",
    "\n",
    "To create a DTM, we will use the `CountVectorizer` from the package `sklearn`, a heavily used machine learning package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the workflow:\n",
    "\n",
    "1. We first create a `CountVectorizer` object, and choose specific settings for how we'll go about creating the DTM. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see what options are available.\n",
    "2. Then, we \"fit\" this `CountVectorizer` object to the data. In this context, \"fitting\" consists of establishing a vocabulary of tokens from the documents in your dataset.\n",
    "3. Finally, we \"transform\" the data according to the \"fitted\" `CountVectorizer` object. This means taking our text data and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "4. You can do steps 2 and 3 in one fell swoop using a `fit_transform` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1639773021096,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "GLxYUFszEA0M",
    "outputId": "ff40b717-b7f4-4710-bd43-a0c7613422f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agree</th>\n",
       "      <th>but</th>\n",
       "      <th>can</th>\n",
       "      <th>cat</th>\n",
       "      <th>does</th>\n",
       "      <th>dog</th>\n",
       "      <th>has</th>\n",
       "      <th>let</th>\n",
       "      <th>likes</th>\n",
       "      <th>my</th>\n",
       "      <th>not</th>\n",
       "      <th>our</th>\n",
       "      <th>out</th>\n",
       "      <th>paws</th>\n",
       "      <th>really</th>\n",
       "      <th>the</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agree  but  can  cat  does  dog  has  let  likes  my  not  our  out  paws  \\\n",
       "0      0    0    0    1     0    0    1    0      0   1    0    0    0     1   \n",
       "1      0    0    1    0     0    1    0    1      0   0    0    0    1     0   \n",
       "2      1    1    0    2     1    1    0    0      1   0    1    1    0     0   \n",
       "\n",
       "   really  the  we  \n",
       "0       0    0   0  \n",
       "1       0    1   1  \n",
       "2       1    2   0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "  'My cat has paws.',\n",
    "  'Can we let the dog out?',\n",
    "  'Our dog really likes the cat but the cat does not agree.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "test_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtaTTaP8EA0M"
   },
   "source": [
    "Each column in the matrix represents a unique word in the vocabulary, while each row represents the document in our dataset. In this case, we have three sentences (i.e. the document), and therefore we three rows. The values in each cell are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsNok5KyEA0M"
   },
   "source": [
    "üîî **Question**: Lets look at the shape of the matrix. How many unique words are there in the matrix?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1639773036420,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6mKvaLCnEA0M",
    "outputId": "259f70fb-d186-4330-d9a7-d726459105fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kPKfAnFvKY"
   },
   "source": [
    "Now, we have numbers representing the contents of the documents! Matrices like this are the simplest way to represent texts. However, it biases most frequent words and ends up ignoring rare words which could have helped is in processing our data more efficiently.\n",
    "\n",
    "Often, we not only want to focus on the frequency of words present in the corpus but also want to know the importance of the words. This is where TF-IDF (term frequency-inverse document frequency) comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpWynSoy4t8o"
   },
   "source": [
    "# Implementing TF-IDF\n",
    "\n",
    "TF-IDF, short for **term frequency‚Äìinverse document frequency**, is a metric that reflects how important a word is to a **document** in a collection or **corpus**. When talking about text datasets, the dataset is called a corpus, and each datapoint is a document. A document can be a post, a paragraph, a webpage, whatever is considered the individual unit of text for a given datset. A **term** is each unique token in a document (we previously also referred to this as **type**). \n",
    "\n",
    "For example in a corpus of sentences, a document might be: `\"I went to New York City in New York state.\"` \n",
    "\n",
    "The processed tokens in that document might be: `[went, new_york, city, new_york, state]`.\n",
    "\n",
    "The document would have four unique terms: `[went, new_york, city, state]`.\n",
    "\n",
    "The TF-IDF value increases proportionally to the number of times a word appears in the document (the term frequency, or TF), and is offset by the number of documents in the corpus that contain the word (the inverse document frequency, or IDF). This helps to adjust for the fact that some words appear more frequently in general ‚Äì such as articles and prepositions.\n",
    "\n",
    "We won't go into much detail about the math behind calculating the TF-IDF (see the D-Lab Text Analysis workshop videos to see more). The key components to remember are:\n",
    "\n",
    "1. There is one TF-IDF score per unique word and unique document.\n",
    "2. A high TF-IDF score suggests that word is descriptive of that document.\n",
    "3. A low TF-IDF score may be because either the word is not frequent in that document, or that it is frequent in many documents in the dataset - either way, it may not be a good descriptor of that document.\n",
    "\n",
    "The intuition is that if a word occurs many times in one post but rarely in the rest of the corpus, it is probably useful for characterizing that post; conversely, if a word occurs frequently in a post but also occurs frequently in the corpus, it is probably less characteristic of that post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:**  Word order is still not retained in this type of featurization, since all that is counted is overall frequency of a word in a document. So it is still a **bag-of-words** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxZ-5f6ZFWe"
   },
   "source": [
    "<a id='toy'></a>\n",
    "\n",
    "# Testing TF-IDF with a Toy Dataset\n",
    "\n",
    "Let's try TF-IDF out with a toy dataset. Here we have three documents about Python, but with different meanings. If we are trying to distinguish between these documents, the word \"Python\" would not be very useful, since it occurs in all of the documents, but other terms might, like \"Monty\", \"snake\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MjjWn-phZFWf"
   },
   "outputs": [],
   "source": [
    "document1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Python, from the Greek word (œÄœçŒ∏œâŒΩ/œÄœçŒ∏œâŒΩŒ±œÇ), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Monty Python (also collectively known as the Pythons) are a British \n",
    "surreal comedy group who created the sketch comedy television show Monty Python's \n",
    "Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made \n",
    "over four series.\"\"\"\n",
    "\n",
    "document4 = \"\"\"Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes \n",
    "code readability with its notable use of significant whitespace. Its language constructs and \n",
    "object-oriented approach aim to help programmers write clear, logical code for small and \n",
    "large-scale projects.\"\"\"\n",
    "\n",
    "document5 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\". It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment.\"\"\"\n",
    "\n",
    "document6 = \"\"\"The Pythonidae, commonly known simply as pythons, from the Greek word python \n",
    "(œÄœÖŒ∏œâŒΩ), are a family of nonvenomous snakes found in Africa, Asia, and Australia. \n",
    "Among its members are some of the largest snakes in the world. Eight genera and 31\n",
    "species are currently recognized.\"\"\"\n",
    "\n",
    "test_list = [document1, document2, document3, document4, document5, document6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESR7eMThTkZE"
   },
   "source": [
    "## Using `CountVectorizer()`\n",
    "\n",
    "Let's use `CountVectorizer()` again, and customize it a bit. \n",
    "\n",
    "We will set the variable `cv` to an instance of `CountVectorizer()`, and change two parameters:\n",
    "    - Set `max_df` (max document frequency) to get rid of words that appear in more than 85% of the corpus. \n",
    "    - Set `stop_words` to include the English stopword list, in order to leave those stopwords out of the calculations as well.\n",
    "    \n",
    "Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) if you need help on how to change these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776660955,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Os8jrh3RM_GJ",
    "outputId": "b7adadfc-b6fa-4752-b8c1-08527bd07703"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `fit_transform()` on our `test_list` with these settings. We'll also create a dataframe with all the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1955</th>\n",
       "      <th>1969</th>\n",
       "      <th>1991</th>\n",
       "      <th>2000</th>\n",
       "      <th>31</th>\n",
       "      <th>357</th>\n",
       "      <th>44</th>\n",
       "      <th>actors</th>\n",
       "      <th>africa</th>\n",
       "      <th>aim</th>\n",
       "      <th>...</th>\n",
       "      <th>wil</th>\n",
       "      <th>william</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>zabka</th>\n",
       "      <th>œÄœÖŒ∏œâŒΩ</th>\n",
       "      <th>œÄœçŒ∏œâŒΩ</th>\n",
       "      <th>œÄœçŒ∏œâŒΩŒ±œÇ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows √ó 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1955  1969  1991  2000  31  357  44  actors  africa  aim  ...  wil  \\\n",
       "0     0     0     0     1   0    0   0       1       0    0  ...    1   \n",
       "1     0     0     0     0   0    0   0       0       1    0  ...    0   \n",
       "2     0     1     0     0   0    0   0       0       0    0  ...    0   \n",
       "3     0     0     1     0   0    0   0       0       0    1  ...    0   \n",
       "4     1     0     0     0   0    1   1       0       0    0  ...    0   \n",
       "5     0     0     0     0   1    0   0       0       1    0  ...    0   \n",
       "\n",
       "   william  word  world  write  year  zabka  œÄœÖŒ∏œâŒΩ  œÄœçŒ∏œâŒΩ  œÄœçŒ∏œâŒΩŒ±œÇ  \n",
       "0        1     0      0      0     0      1      0      0        0  \n",
       "1        0     1      0      0     0      0      0      1        1  \n",
       "2        0     0      0      0     0      0      0      0        0  \n",
       "3        0     0      0      1     0      0      0      0        0  \n",
       "4        0     0      0      0     1      0      0      0        0  \n",
       "5        0     1      1      0     0      0      1      0        0  \n",
       "\n",
       "[6 rows x 147 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector = cv.fit_transform(test_list)\n",
    "pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkpVqpPzNHRd"
   },
   "source": [
    "üîî **Question**: How many documents and unique words are in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLHjD29ZFWi"
   },
   "source": [
    "## Using `TfidfTransformer`\n",
    "\n",
    "Next, we need to compute the inverse document frequency values. We'll call [`tfidf_transformer.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) on the word counts we computed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776806708,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "WeTAynM9xxxr",
    "outputId": "a5c7fd51-b4cf-4c16-e643-388303891e01"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer() \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uoj5xOi8vu_"
   },
   "source": [
    "To get a glimpse of how the IDF values look, let's put these into a DataFrame and sort by weights. Remember, a low IDF indicates something that is less unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1639776807884,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "gRjP38dV-7I7",
    "outputId": "ba06ac97-0405-47f1-d01d-db1bbf5ca801"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>known</th>\n",
       "      <td>1.336472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythons</th>\n",
       "      <td>1.559616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonvenomous</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>films</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fame</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>œÄœçŒ∏œâŒΩŒ±œÇ</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf_weights\n",
       "known           1.336472\n",
       "pythons         1.559616\n",
       "greek           1.847298\n",
       "nonvenomous     1.847298\n",
       "created         1.847298\n",
       "...                  ...\n",
       "film            2.252763\n",
       "films           2.252763\n",
       "flying          2.252763\n",
       "fame            2.252763\n",
       "œÄœçŒ∏œâŒΩŒ±œÇ         2.252763\n",
       "\n",
       "[147 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print IDF values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=[\"idf_weights\"]) \n",
    "# Sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFeXhx71O_4n"
   },
   "source": [
    "Notice that the words \"python\" and \"in\" have the lowest IDF values. This is expected: these words appear in each and every document in our collection. The lower the IDF value of a word, the less unique it is to any particular document.\n",
    "\n",
    "Now that we have the idf values, we can compute the TF-IDF scores for our set of documents using `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hWPqOTR_AIM0"
   },
   "outputs": [],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0-6H9L9PnLA"
   },
   "source": [
    "By invoking `tfidf_transformer.transform()` we are computing the TF-IDF scores for our docs. Internally, this is weighting TF scores by their IDF scores, so that the more unique a word, the more its frequency counts in a given document.\n",
    "\n",
    "Let‚Äôs print the TF-IDF values of the first document to see if it makes sense. We place the TF-IDF scores from the third document into a `pandas` data frame and sort it in descending order of scores. We can replace the index in `tf_idf_vector[]` to select a different document to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1639776812749,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Qu40IZhpP0b2",
    "outputId": "fbf33f79-7be4-4c0e-8853-02ab6b7cf616"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comedy</th>\n",
       "      <td>0.424705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monty</th>\n",
       "      <td>0.424705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circus</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surreal</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>englund</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emphasizes</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discontinued</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>œÄœçŒ∏œâŒΩŒ±œÇ</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "comedy        0.424705\n",
       "monty         0.424705\n",
       "circus        0.212353\n",
       "television    0.212353\n",
       "surreal       0.212353\n",
       "...                ...\n",
       "englund       0.000000\n",
       "emphasizes    0.000000\n",
       "elm           0.000000\n",
       "discontinued  0.000000\n",
       "œÄœçŒ∏œâŒΩŒ±œÇ       0.000000\n",
       "\n",
       "[147 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names_out() \n",
    "  \n",
    "# Print the scores \n",
    "df_test = pd.DataFrame(tf_idf_vector[2].T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df_test.sort_values(by=[\"tfidf\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9qVYEm_ZFW2"
   },
   "source": [
    "What are the most distinctive words for document 3 (Highest TF-IDF scores?) Does it made sense given the document and corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq3QvXEcZFXS"
   },
   "source": [
    "<a id='reddit'></a>\n",
    "\n",
    "# Using TF-IDF on Reddit Datasets\n",
    "\n",
    "Now that we have a good grasp of how TF-IDF is calculated, let's perform this method on our Reddit data. This time, to save time, we will be using `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer). It is a class that basically allows us to create a matrix of word counts (what we just did with `CountVectorizer`), and immediately transform them into TF-IDF values.\n",
    "\n",
    "We simply instantiate an object of the `TfidfVectorizer`. Then, we run it by applying the `fit_transform()` method to our two-document corpus `aita_tokens`.\n",
    "\n",
    "Note that we are setting `max_features` to 1000. This means we will only create tf-idf values for the top-1000 most-frequent terms in our corpus. In other words, our vocabulary will be 1000 in length.\n",
    "\n",
    "üí≠ **Reflection**: Think about why getting rid of infrequent terms could be helpful, and what the potential risks could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1vRKbLpDZFXY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                   max_features=1000,\n",
    "                                   decode_error='ignore',\n",
    "                                   stop_words='english',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf = tfidf_vectorizer.fit_transform(aita_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tfidf` object we created is a so-called **sparse matrix**. We will first need to convert this into something we can read and make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16313x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1148408 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"sparse matrix\"\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this sparse matrix into a DataFrame. We first convert `tfidf` into a **dense matrix** using the `todense()` method, then pipe that into a `pd.DataFrame` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1639778604305,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cghW5hxxZ8Ec",
    "outputId": "d6b9cf01-37c7-4fc1-dc6d-3c8aff1a6e16"
   },
   "outputs": [],
   "source": [
    "# Place TF-IDF values in a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>30</th>\n",
       "      <th>50</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>www</th>\n",
       "      <th>x200b</th>\n",
       "      <th>yard</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yell</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000   30   50  able  abortion  absolutely  abuse  accept  access  accident  \\\n",
       "0  0.0  0.0  0.0   0.0       0.0    0.000000    0.0     0.0     0.0       0.0   \n",
       "1  0.0  0.0  0.0   0.0       0.0    0.093202    0.0     0.0     0.0       0.0   \n",
       "2  0.0  0.0  0.0   0.0       0.0    0.000000    0.0     0.0     0.0       0.0   \n",
       "3  0.0  0.0  0.0   0.0       0.0    0.000000    0.0     0.0     0.0       0.0   \n",
       "4  0.0  0.0  0.0   0.0       0.0    0.000000    0.0     0.0     0.0       0.0   \n",
       "\n",
       "   ...     wrong  www  x200b  yard  yeah      year  yell       yes  yesterday  \\\n",
       "0  ...  0.034653  0.0    0.0   0.0   0.0  0.101543   0.0  0.084721   0.000000   \n",
       "1  ...  0.000000  0.0    0.0   0.0   0.0  0.041480   0.0  0.000000   0.000000   \n",
       "2  ...  0.051734  0.0    0.0   0.0   0.0  0.000000   0.0  0.000000   0.000000   \n",
       "3  ...  0.000000  0.0    0.0   0.0   0.0  0.000000   0.0  0.000000   0.123718   \n",
       "4  ...  0.000000  0.0    0.0   0.0   0.0  0.151790   0.0  0.000000   0.000000   \n",
       "\n",
       "      young  \n",
       "0  0.000000  \n",
       "1  0.158115  \n",
       "2  0.000000  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this `tfidf_idf` is a lot like a Document-Term Matrix, except with Tf-IDF counts! The columns represent every term in our 1000-word vocabulary. The rows represent documents in which these words appear. \n",
    "\n",
    "Note that most values are 0. This is because most words in our vocabulary actually don't show up in most documents.\n",
    "\n",
    "We can retrieve the highest TF-IDF values across documents in this DataFrame by just summing all TF-IDF values, and then calling `.sort_values()` on our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tell      1010.209647\n",
       "want       805.850508\n",
       "like       746.942365\n",
       "ask        659.519182\n",
       "think      635.543366\n",
       "             ...     \n",
       "peace       33.942972\n",
       "style       33.883993\n",
       "aside       33.498759\n",
       "longer      33.028562\n",
       "sam         30.919569\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highest TF-IDF values across documents\n",
    "tfidf_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YoFv8pXZFXa"
   },
   "source": [
    "<a id='similar'></a>\n",
    "\n",
    "## Using TF-IDF to Find Similar Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjvztHrBZFXb"
   },
   "source": [
    "We can use TF-IDF to work out the similarity between any pair of documents. So given one post or comment, we could see which posts or comments are most similar. This can be useful if you're trying to find other examples of a pattern you have found and want to explore further.\n",
    "\n",
    "This time, our \"documents\" will not be entire subreddits, but posts/submissions within one subreddit. Let's import the submissions and run the vectorizer without the preprocessing and lemmatizing. Tf-idf will still work this way, and this way, we will be able to read our posts.\n",
    "\n",
    "We'll use the TF-IDF vectorized output from the previous section. Let's choose a particular document, and try and find similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s 6PM on a Friday, store has hundreds of people, there are only 3 registers open and lines are ridiculously long. The self checkout has a line that wraps but its the fastest moving line so we wait about 15 minutes in line to check ourselves out.\\n\\nAfter we check out, a line is forming to exit the building because everyone is waiting for the walmart receipt checker to glance at their receipt.\\n\\nI\\'m already frustrated because of the wait so I skip the line, and the checker anxiously tries to get my attention and loudly says \"SIR I NEED TO CHECK YOUR RECEIPT!\", I respond with a loud(because its loud in the store) \"NO THANK YOU\", and walk out of the building.  People start following my example.\\n\\nThus ensues a 15 minute fight in the car with the wife because she feels I made a scene.\\n\\nEDIT: Well this turned out well.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selftext'].iloc[doc_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question:** What is this post about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6l6lqgF4B0L"
   },
   "source": [
    "Let's have a quick look at the TF-IDF scores for the words in this submission to see if these words are indeed typical for this particular submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639779140786,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rpOde4Sr4AzJ",
    "outputId": "8beb14d4-9dab-4266-8f69-4a9ca73259cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line           0.660411\n",
       "check          0.309568\n",
       "wait           0.289944\n",
       "loud           0.267614\n",
       "store          0.249029\n",
       "                 ...   \n",
       "favorite       0.000000\n",
       "feed           0.000000\n",
       "feel_guilty    0.000000\n",
       "feeling        0.000000\n",
       "young          0.000000\n",
       "Name: 25, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.loc[doc_idx].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question:** Do the distinctive words have to do with the topic of the post?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiTzCg4xZFXi"
   },
   "source": [
    "## Calculating similarity \n",
    "\n",
    "Now let's find the closest posts to this one. The fact that our documents are now in a vector space allows us to make use of mathematical similarity metrics.\n",
    "\n",
    "**Cosine similarity** is one metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. It is equal to 1 if the documents are the same, and decreases to 0 the more dissimilar they are.\n",
    "\n",
    "We can use a cosine similarity function from `sklearn` to calculate the cosine similarity between each pair of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Rbfy9VoCEiPc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16313, 16313)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(tfidf)\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the text and scores in a dataframe, and sort by the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_df = pd.DataFrame({\n",
    "    'text': df['selftext'].values,\n",
    "    'score': similarities[doc_idx]}).sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top document will be the document itself (it's going to have a similarity of 1 with itself). So we look at the next document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1639779186306,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "-dEbe-PoIizd",
    "outputId": "012bdd5a-c563-4a73-9806-9ee882836949"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s 6PM on a Friday, store has hundreds of people, there are only 3 registers open and lines are ridiculously long. The self checkout has a line that wraps but its the fastest moving line so we wait about 15 minutes in line to check ourselves out.\\n\\nAfter we check out, a line is forming to exit the building because everyone is waiting for the walmart receipt checker to glance at their receipt.\\n\\nI\\'m already frustrated because of the wait so I skip the line, and the checker anxiously tries to get my attention and loudly says \"SIR I NEED TO CHECK YOUR RECEIPT!\", I respond with a loud(because its loud in the store) \"NO THANK YOU\", and walk out of the building.  People start following my example.\\n\\nThus ensues a 15 minute fight in the car with the wife because she feels I made a scene.\\n\\nEDIT: Well this turned out well.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I (25f) was at vons pretty late and I had a whole cart full of things, hadn‚Äôt eaten all day so hangry and annoyed, no food at home, exhausted, going through the mental grocery list and what the total should be‚Ä¶all the usual store stuff‚Ä¶ Keep in mind I try to get as much at once as possible and meal prep so I don‚Äôt have to go to the store because it sucks and I hate it..as I was loading it all onto the conveyor this girl a few years younger than me asked if she could cut in line because she only had one thing of soda and booze (which you can‚Äôt go to self checkout for). I said no and turned back to my groceries, she looked extremely offended and scoffed at me and stomped off. \\nThen the person ringing me up started to shame me. And not just a little like ranted the whole time he was ringing me up, how rude that was. I didn‚Äôt say anything but I was so mad I was shaking‚Ä¶I stood in line and waited my turn?? There were other lines open AND would you do that at the DMV? Or anywhere else? It was such a weird thing to get shamed for because that‚Äôs the point of a line?? Idk it made me feel like an asshole but if the grocery store worked like that it‚Äôd be chaos‚Ä¶and why not just go to a liquor store right NEXT TO the Vons?!? Where people with a zillion items aren‚Äôt taking up the line???'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_df['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí≠ **Reflection**: Think about why this document seems similar, in terms of TF-IDF scores, to the one we pulled out first. Do you agree with this calculation of similarity? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using TF-IDF to Find Posts\n",
    "\n",
    "TF-IDF has been used in search algorithms. This makes sense: after all, it can tell you which words are uncommonly frequent in some text. In this sense, TF-IDF can act as a keyword or topic identifier.\n",
    "\n",
    "For instance, if we would want to look for a text in our DF that has a high TF-IDF score for the word \"husband\", we could create a **boolean mask** of our `tfidf_df`. Below, we only select those rows where the column \"husband\" has a TF-IDF score higher than `.5`. We can then use that mask to subset our original `df`! This is because the rows in both DataFrames refer to the same thing: our Reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>130</td>\n",
       "      <td>625049650</td>\n",
       "      <td>t3_ac4zea</td>\n",
       "      <td>1546515991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmehex</td>\n",
       "      <td>AITA for not wanting my husband‚Äôs best friend ...</td>\n",
       "      <td>My husband and I just got into (another) fight...</td>\n",
       "      <td>9595.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6009.0</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>3729</td>\n",
       "      <td>843478036</td>\n",
       "      <td>t3_dy6ntg</td>\n",
       "      <td>1574099693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>throwaway2340821</td>\n",
       "      <td>AITA for referring to my soon-to-be-ex husband...</td>\n",
       "      <td>To make this very short:\\n\\nMy husband and I a...</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>5208</td>\n",
       "      <td>920488634</td>\n",
       "      <td>t3_f819kq</td>\n",
       "      <td>1582415992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zina-winter</td>\n",
       "      <td>AITA for wanting to be buried with my first hu...</td>\n",
       "      <td>I am 31F, married for 2 years to my husband, 3...</td>\n",
       "      <td>4124.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      idint      idstr     created  nsfw            author  \\\n",
       "127          130  625049650  t3_ac4zea  1546515991   0.0            mmehex   \n",
       "3308        3729  843478036  t3_dy6ntg  1574099693   0.0  throwaway2340821   \n",
       "4559        5208  920488634  t3_f819kq  1582415992   0.0       zina-winter   \n",
       "\n",
       "                                                  title  \\\n",
       "127   AITA for not wanting my husband‚Äôs best friend ...   \n",
       "3308  AITA for referring to my soon-to-be-ex husband...   \n",
       "4559  AITA for wanting to be buried with my first hu...   \n",
       "\n",
       "                                               selftext   score distinguish  \\\n",
       "127   My husband and I just got into (another) fight...  9595.0         NaN   \n",
       "3308  To make this very short:\\n\\nMy husband and I a...  1584.0         NaN   \n",
       "4559  I am 31F, married for 2 years to my husband, 3...  4124.0         NaN   \n",
       "\n",
       "      textlen  num_comments      flair_text flair_css_class  \n",
       "127    6009.0        1272.0  Not the A-hole             not  \n",
       "3308    796.0         533.0             NaN             NaN  \n",
       "4559   1401.0         645.0  Not the A-hole             not  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsetting one DF with the mask of another DF\n",
    "tfidf_husband_df = df[tfidf_df['husband'] > .5]\n",
    "tfidf_husband_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the \"selftext\" column from this new subsetted DataFrame, and then get the first post just to have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My husband and I just got into (another) fight because his best friend who comes over every Wednesday is (again) staying the night. \n",
      "\n",
      "My husband and this friend have had a Wednesday night ‚Äúdate‚Äù where the guy comes over to our house to ‚Äújam‚Äù every week for the last two years, and I‚Äôm getting to the point that I want to hit the guy over the head with whatever is closest at hand every time he is around. \n",
      "\n",
      "Buckle up. This is probably going to be a long one.... if you make it through all of this you are my hero. \n",
      "\n",
      "At first he was supposed to come over just for a few hours on Wednesday nights to play guitar with my husband while I was at work. Over time I have stopped working late Wednesday‚Äôs but the jam session continues. Which was fine. However, for the last year they have barely touched the guitars and instead the guy just comes over to get high (on pot). Neither my husband or I get high with him (I don‚Äôt enjoy pot and my husband isn‚Äôt much into it either, but we aren‚Äôt morally against it so we didn‚Äôt say anything early on when we should have). \n",
      "\n",
      "Now that he is always high here, he NEVER leaves at the end of the night. Just expects to sleep over. But he doesn‚Äôt have a regular sleep schedule so instead of sleeping while we are, he is wandering around our house all night (in his boxers) making noises, watching tv, getting nonstop text alerts from other friends and ect. This always wakes me up and I have to pass by him on the way to the bathroom. At these points he always has demands. Like: ‚ÄúI want to watch this random movie your husband said you had. Find it for me.‚Äù Or ‚Äúyou need to change your litter box right now, because your cat just pooped and I can‚Äôt stand the smell.‚Äù He always says he‚Äôll leave when my husband goes for work at 8am, but then complains that he couldn‚Äôt sleep until 6am, so he ends up sleeping until mid day. At which point he just decides to stay until my husband gets home from work and then the whole thing starts over again. What‚Äôs supposed to be one night a week often turns into at least two every week. \n",
      "\n",
      "On top of never leaving, he has started showing up HOURS before my husband gets off work on Wednesday‚Äôs. My husband has told him ‚Äúmy home is your home‚Äù so he just comes right in. This wouldn‚Äôt be a problem if he could self entertain. But no. He comes in and will just walk into our bedroom (where I‚Äôm hiding) sit down on the bed, take a hit of his shit, and start talking to me through whatever I‚Äôm doing/watching on tv. \n",
      "\n",
      "Every time he is here he eats A LOT of our food (gets the munchies, plus regular meals) but never once has had the courtesy to bring anything of his own, or to share, or to replace what he is eating here. He just goes into our fridge and pantry and eats whatever he wants because my husband told him our home was his. \n",
      "\n",
      "My husband is always begging me to be nice and not say anything while he is here because he is lonely and needs friends.... except he has his other best friend living with him, and is constantly going on weekend trips all over the country to visit his other friends.  Not to mention those nonstop text message alerts all night. \n",
      "\n",
      "Dealing with this guy is like hanging out with a 17 year old stoner non stop. He farts and burps as loud as possible wherever. Whenever. Is demanding and needy, but completely rude at the same time. Saying things like ‚Äúdude! You actually made your house look SORTA nice this week.‚Äù (As I was prepping for Christmas), and ‚Äúso you do sometimes make an effort on your looks‚Äù (when I had gone somewhere nice and come home with makeup on). He also complains non stop about the food I make (yet eats it all anyway). \n",
      "\n",
      "This year for my birthday we couldn‚Äôt afford to get any gifts, so I asked my husband instead to line up his days off so we could spend four days just us together...... then this guy broke up with his girlfriend and my husband begged me to let him come over just for a bit on the first evening. Que TWO full weeks of him not going home, and demanding all of my husband‚Äôs time. I spent ‚Äúour time‚Äù and my birthday, holed up in our room alone. \n",
      "\n",
      "Then, Christmas comes. I had invited my whole family to our house and tried to make things extra special because it was the first time I got to see my parents on Christmas in more than ten years. This guy‚Äôs mom had just moved to another state BUT this guy has plenty of money and no job to work around so he could have easily made the trip to her for Christmas. But he didn‚Äôt. Then his other friend/housemate also went out of town and he threw a ‚Äúwoe is me, I‚Äôm all alone one Christmas Eve, no one loves me‚Äù tantrum until my husband invited him to our house. He shows up. Gets high. And is a complete ass to everyone. Then throws a fit when he decides to sleep over because he has to sleep on the couch in a sleeping bag (my family were in the other beds.) the next morning we sit down to breakfast and he lets out this HUGE stinky fart right at the table like it‚Äôs nothing, then proceeds to dish himself up 90% of the food and just generally be a douche. My family was flat out disgusted by him and his behavior and couldn‚Äôt wait to leave. He stayed for three days. \n",
      "\n",
      "I feel like an ass hole because he is my husband‚Äôs best friend and I don‚Äôt want to make them not hang out, and honestly he hasn‚Äôt done anything truly terrible. He‚Äôs just annoying as shit. I fucking dread every Wednesday, and then I‚Äôm super irritated the whole time he‚Äôs here. Which I hide from him, at my husband‚Äôs request, but then my husband and I always end up fighting over him not leaving at the end of the night. \n",
      "\n",
      "I‚Äôm honestly one or two Wednesday‚Äôs away from homicide with this guy, and all this I‚Äôve told you is just the tip of the iceberg. \n",
      "\n",
      "If you‚Äôve made it this far, please tell me AITA for being upset at my husband for letting this guy stay over again tonight, and wanting a substantial break from his best friend? \n",
      "\n",
      "\n",
      "\n",
      "TL;DR My husband‚Äôs best friend is a rude stoner who never leaves. AITA for not wanting him to come over for a LONG time? \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thank you all for your overwhelming response!! I haven‚Äôt been able to reply to all of you, but I want you to know I‚Äôve read every post and I appreciate all of your opinions (even the ones calling me a shrew) and your great advice! \n",
      "\n",
      "Some clarification and an update: In case you weren't already sick of reading all this >< \n",
      "\n",
      "\n",
      "Clarification 1: As some have pointed out, there isn‚Äôt much of an action here for me to ask AITA about. I‚Äôm sorry I didn‚Äôt make it clearer originally. The question is am I unreasonable and being an Ass Hole for wanting a break from having this friend over. My husband and I fought over this last night and he feels I‚Äôm being overly upset and needlessly irritable at said friend. You‚Äôve already read how I feel about it. \n",
      "\n",
      "\n",
      "Clarification 2: This post, admittedly doesn't give much positive about the friend. His friend has been in my husband‚Äôs life since he was in grade school, and while the friend is over they do actively chat, watch movies, and generally hang out while I actively hide and feel irritated. All this stuff for sure could look like an exaggeration if it all happened in the last few days, but this is sorta my laundry list of the last two years of grievances which have been building up over time. On a regular Wednesday he is just stoned, flatulent, and childish, which while annoying, isn‚Äôt really actively douchy. He has done some very nice things for my husband, such as paying to take him to some very expensive concerts so that they could both nerd out over their favorite guitar players Live. I -clearly- don‚Äôt like the guy, but my husband gets a lot of enjoyment out of their friendship and I‚Äôm not saying that has to stop. Just that -I- need a break. Also, my husband DOES have other friends, they just have lives so he only gets to hang out once every few months with them. He is also in a band with other friends whom he sees regularly every Monday night. \n",
      "\n",
      "Clarification 3: My husband doesn‚Äôt see a problem with how he speaks to me because according to him ‚Äúthe guy just has poor social skills, that comment about your makeup was a compliment.‚Äù\n",
      "\n",
      "Clarification 4: He smokes a vape style pot pen, not a physical bud, which has less residual scent and is why my husband told him he could do it in the house. Before he started using I was ok with him. He still had poor manners, but he wasn‚Äôt nearly as bad as he is now, and he never spent the night (or longer). Back before the pot I even enjoyed hanging out with the two of them every so often. Oh and I KNOW my husband occasionally also imbibes with him, which isn‚Äôt something that makes me mad. He doesn‚Äôt go overboard like his friend. \n",
      "\n",
      "UPDATE: When I woke up this morning I noticed that my phone was buzzing‚Ä¶. And buzzing‚Ä¶. And buzzing‚Ä¶. As all these awesome comments kept pouring in. I have never really had much success on Reddit, so when I looked and saw how many comments there were I said ‚ÄúOh My GoD!‚Äù Of course my husband asked what was wrong and I said ‚ÄúI told the internet the story of ***** and asked if I was the ass hole for what we fought over last night.‚Äù He said..‚ÄùOh‚Äù and that was it. BUT then on his way out of the bedroom he said ‚ÄúI‚Äôm going to get him up‚Äù, and by the time I was dressed and out of the room they guy was out of the house and driving away. Hallelujah!!! \n",
      "\n",
      "Forty minutes later when he got to work I received a text message telling me how I was his best friend and that he loves me more than the world ect. Then we texted back and forth and he said ‚ÄúI really feel like we both contribute to it. I feel like you are overly grumpy on things pertaining to *****. But at the same time, I invite him to come up extra days, or stay the night without running it by you. I feel we could both do better and relieve a lot of stress and frustration. Do you agree?‚Äù  \n",
      "\n",
      "He then called a few minutes later and I told him that I know I‚Äôm grumpy but I feel like it‚Äôs reasonably so. I also told him I didn‚Äôt want to force them not to be friends just that -I- needed a break from the friend. Then I gave him some possible solutions such as hanging out at his house every other week, or have a set time that the guy needs to stop smoking in order to drive home later (thanks guys! Great suggestions!) and he agreed to consider other arrangements with the hang out time so that I could have some time away from the friend, and we agreed to talk more about it when he is off work tonight. Although I can tell he STILL isn‚Äôt seeing a problem other than my attitude : / \n",
      "\n",
      "Again thank you to everyone who has commented! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TL;DR My husband‚Äôs best friend is a rude stoner who never leaves. AITA for not wanting him to come over for a LONG time? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_husband_df['selftext'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF Correlations to Explore Biases\n",
    "\n",
    "Calculating correlations of TF-IDF values can be a useful technique to explore relationships between words or terms in a corpus. By analyzing the correlations, we can identify whether certain words tend to appear together more frequently or if they are related to similar topics. \n",
    "\n",
    "Pandas allows us to trace pairwise correlations in a DataFrame using the `corr()` method. Note that this creates a new DataFrame that is square and symmetric. Each element in the resulting DataFrame represents the correlation coefficient between all terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr = tfidf_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>30</th>\n",
       "      <th>50</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>www</th>\n",
       "      <th>x200b</th>\n",
       "      <th>yard</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yell</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>-0.005835</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.025329</td>\n",
       "      <td>0.023042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020560</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>-0.004301</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>-0.012509</td>\n",
       "      <td>0.045526</td>\n",
       "      <td>-0.020144</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>-0.008738</td>\n",
       "      <td>0.003732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.009099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.007406</td>\n",
       "      <td>-0.012792</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012593</td>\n",
       "      <td>-0.000532</td>\n",
       "      <td>-0.007549</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>-0.024249</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.008382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015677</td>\n",
       "      <td>-0.011046</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-0.011919</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>-0.006988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019852</td>\n",
       "      <td>-0.014205</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>-0.011951</td>\n",
       "      <td>-0.012402</td>\n",
       "      <td>0.028489</td>\n",
       "      <td>-0.011778</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.016625</td>\n",
       "      <td>-0.015747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.015677</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008622</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.021316</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009869</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.007740</td>\n",
       "      <td>-0.001868</td>\n",
       "      <td>-0.011171</td>\n",
       "      <td>0.059458</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>-0.008869</td>\n",
       "      <td>-0.014777</td>\n",
       "      <td>0.004079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abortion</th>\n",
       "      <td>-0.005835</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>-0.011046</td>\n",
       "      <td>0.008622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>-0.010999</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.009376</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.014321</td>\n",
       "      <td>-0.010116</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>-0.006314</td>\n",
       "      <td>-0.003507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.045526</td>\n",
       "      <td>-0.024249</td>\n",
       "      <td>0.028489</td>\n",
       "      <td>0.059458</td>\n",
       "      <td>0.014321</td>\n",
       "      <td>0.013580</td>\n",
       "      <td>0.018976</td>\n",
       "      <td>0.046779</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025150</td>\n",
       "      <td>-0.045627</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>-0.020156</td>\n",
       "      <td>-0.012775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030246</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>-0.047221</td>\n",
       "      <td>0.080620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yell</th>\n",
       "      <td>-0.020144</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>-0.011778</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>-0.010116</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>-0.012526</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040292</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>0.015078</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>-0.030246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>0.003820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>-0.018294</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.008869</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>-0.004361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.010090</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016064</td>\n",
       "      <td>-0.008713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yesterday</th>\n",
       "      <td>-0.008738</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.016625</td>\n",
       "      <td>-0.014777</td>\n",
       "      <td>-0.006314</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>-0.001286</td>\n",
       "      <td>-0.003129</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015531</td>\n",
       "      <td>-0.016587</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>-0.047221</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>0.016064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>0.003732</td>\n",
       "      <td>-0.008382</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>-0.001672</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.037689</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>-0.007918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007859</td>\n",
       "      <td>-0.014113</td>\n",
       "      <td>-0.003549</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.010200</td>\n",
       "      <td>0.080620</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.017602</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                000        30        50      able  abortion  absolutely  \\\n",
       "000        1.000000  0.009099  0.011782  0.036242 -0.005835    0.003298   \n",
       "30         0.009099  1.000000  0.023077  0.007315 -0.004134   -0.008507   \n",
       "50         0.011782  0.023077  1.000000  0.015677 -0.011046    0.005990   \n",
       "able       0.036242  0.007315  0.015677  1.000000  0.008622    0.003006   \n",
       "abortion  -0.005835 -0.004134 -0.011046  0.008622  1.000000    0.013038   \n",
       "...             ...       ...       ...       ...       ...         ...   \n",
       "year       0.045526 -0.024249  0.028489  0.059458  0.014321    0.013580   \n",
       "yell      -0.020144 -0.001285 -0.011778 -0.022758 -0.010116    0.002202   \n",
       "yes       -0.018294  0.001950 -0.010042 -0.008869  0.005516    0.008589   \n",
       "yesterday -0.008738 -0.000415 -0.016625 -0.014777 -0.006314    0.003153   \n",
       "young      0.003732 -0.008382 -0.015747  0.004079 -0.003507   -0.001672   \n",
       "\n",
       "              abuse    accept    access  accident  ...     wrong       www  \\\n",
       "000        0.000173  0.007959  0.025329  0.023042  ... -0.020560  0.003016   \n",
       "30        -0.007406 -0.012792  0.005005 -0.004776  ... -0.012593 -0.000532   \n",
       "50        -0.000520 -0.011919  0.000905 -0.006988  ... -0.019852 -0.014205   \n",
       "able       0.009161  0.021316  0.037528  0.005209  ... -0.009869  0.007405   \n",
       "abortion   0.001460  0.008893  0.001946  0.008844  ...  0.004469 -0.010999   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "year       0.018976  0.046779  0.008973  0.004231  ... -0.025150 -0.045627   \n",
       "yell       0.015301 -0.012526  0.000847  0.007929  ...  0.040292 -0.007589   \n",
       "yes        0.002192 -0.002229  0.004216 -0.004361  ...  0.014806  0.001663   \n",
       "yesterday -0.001286 -0.003129 -0.001196  0.006012  ...  0.015531 -0.016587   \n",
       "young      0.005379  0.037689 -0.001586 -0.007918  ...  0.007859 -0.014113   \n",
       "\n",
       "              x200b      yard      yeah      year      yell       yes  \\\n",
       "000       -0.004301  0.001766 -0.012509  0.045526 -0.020144 -0.018294   \n",
       "30        -0.007549  0.002524  0.001469 -0.024249 -0.001285  0.001950   \n",
       "50         0.005321 -0.011951 -0.012402  0.028489 -0.011778 -0.010042   \n",
       "able       0.007740 -0.001868 -0.011171  0.059458 -0.022758 -0.008869   \n",
       "abortion  -0.003498 -0.009376 -0.000504  0.014321 -0.010116  0.005516   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "year       0.010854 -0.020156 -0.012775  1.000000 -0.030246 -0.006606   \n",
       "yell      -0.013801  0.015078 -0.000993 -0.030246  1.000000  0.005039   \n",
       "yes       -0.001580 -0.010090  0.010976 -0.006606  0.005039  1.000000   \n",
       "yesterday -0.000700  0.008428  0.011379 -0.047221  0.034983  0.016064   \n",
       "young     -0.003549 -0.003043 -0.010200  0.080620  0.003820 -0.008713   \n",
       "\n",
       "           yesterday     young  \n",
       "000        -0.008738  0.003732  \n",
       "30         -0.000415 -0.008382  \n",
       "50         -0.016625 -0.015747  \n",
       "able       -0.014777  0.004079  \n",
       "abortion   -0.006314 -0.003507  \n",
       "...              ...       ...  \n",
       "year       -0.047221  0.080620  \n",
       "yell        0.034983  0.003820  \n",
       "yes         0.016064 -0.008713  \n",
       "yesterday   1.000000 -0.017602  \n",
       "young      -0.017602  1.000000  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this new DataFrame to compare two concepts in our data. We will look for the columns of \"husband\" and \"wife\" ‚Äì two concepts we expect to appear in this dataset, and that we could imagine being related to gender bias. We then sort the values of the resulting DataFrame based on the \"wife\" columns in descending order, and print the first 20 values. \n",
    "\n",
    "These will be the top-20 words that are most strongly correlated to the term \"wife\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>husband</th>\n",
       "      <th>wife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wife</th>\n",
       "      <td>-0.075467</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>0.077062</td>\n",
       "      <td>0.180826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>son</th>\n",
       "      <td>0.120479</td>\n",
       "      <td>0.098712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divorce</th>\n",
       "      <td>0.047348</td>\n",
       "      <td>0.084186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marry</th>\n",
       "      <td>0.108318</td>\n",
       "      <td>0.084090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>married</th>\n",
       "      <td>0.092746</td>\n",
       "      <td>0.075937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid</th>\n",
       "      <td>0.088507</td>\n",
       "      <td>0.069470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marriage</th>\n",
       "      <td>0.076836</td>\n",
       "      <td>0.064695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>0.060212</td>\n",
       "      <td>0.063105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.075319</td>\n",
       "      <td>0.060563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ex</th>\n",
       "      <td>0.025855</td>\n",
       "      <td>0.058207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>law</th>\n",
       "      <td>0.129579</td>\n",
       "      <td>0.057669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affair</th>\n",
       "      <td>0.039082</td>\n",
       "      <td>0.055666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child</th>\n",
       "      <td>0.094226</td>\n",
       "      <td>0.053233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.054322</td>\n",
       "      <td>0.052868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother</th>\n",
       "      <td>0.043812</td>\n",
       "      <td>0.047106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birth</th>\n",
       "      <td>0.051424</td>\n",
       "      <td>0.045959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnancy</th>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.042859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>0.096673</td>\n",
       "      <td>0.042569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnant</th>\n",
       "      <td>0.076959</td>\n",
       "      <td>0.042011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>father</th>\n",
       "      <td>0.035323</td>\n",
       "      <td>0.040521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision</th>\n",
       "      <td>0.013112</td>\n",
       "      <td>0.039627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom</th>\n",
       "      <td>-0.023184</td>\n",
       "      <td>0.038977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>0.062407</td>\n",
       "      <td>0.038108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ago</th>\n",
       "      <td>0.020058</td>\n",
       "      <td>0.037944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>household</th>\n",
       "      <td>0.019901</td>\n",
       "      <td>0.037314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current</th>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.037061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>care</th>\n",
       "      <td>0.023342</td>\n",
       "      <td>0.036847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fight</th>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.036628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>therapy</th>\n",
       "      <td>0.013922</td>\n",
       "      <td>0.036414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            husband      wife\n",
       "wife      -0.075467  1.000000\n",
       "daughter   0.077062  0.180826\n",
       "son        0.120479  0.098712\n",
       "divorce    0.047348  0.084186\n",
       "marry      0.108318  0.084090\n",
       "married    0.092746  0.075937\n",
       "kid        0.088507  0.069470\n",
       "marriage   0.076836  0.064695\n",
       "home       0.060212  0.063105\n",
       "year       0.075319  0.060563\n",
       "ex         0.025855  0.058207\n",
       "law        0.129579  0.057669\n",
       "affair     0.039082  0.055666\n",
       "child      0.094226  0.053233\n",
       "old        0.054322  0.052868\n",
       "mother     0.043812  0.047106\n",
       "birth      0.051424  0.045959\n",
       "pregnancy  0.062814  0.042859\n",
       "family     0.096673  0.042569\n",
       "pregnant   0.076959  0.042011\n",
       "father     0.035323  0.040521\n",
       "decision   0.013112  0.039627\n",
       "mom       -0.023184  0.038977\n",
       "stay       0.062407  0.038108\n",
       "ago        0.020058  0.037944\n",
       "household  0.019901  0.037314\n",
       "current    0.021682  0.037061\n",
       "care       0.023342  0.036847\n",
       "fight      0.014980  0.036628\n",
       "therapy    0.013922  0.036414"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr[['husband','wife']].sort_values(by='wife',ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ Reflection: \n",
    "\n",
    "- Do these related terms make sense? \n",
    "- Do you see some terms that could be indicative of a bias towards women in the data? \n",
    "- What happens if you change the `by=` sortation to `men`? What words appear now, and how do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Term Frequency, or TF, reflects how often a unique token appears in a corpus.\n",
    "* Inverse Document Frequency (IDF) reflects the number of documents in the corpus that contain a term. \n",
    "* The TF-IDF score of a word reflects how important that word is to a document in a collection or corpus.\n",
    "* Methods from `scikit-learn` can be used to generate TF and TF-IDF scores for a given corpus.\n",
    "* TF-IDF scores can be used to calculate how similar documents are. We can do this with mathematical similarity metrics, such as cosine similarity.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
