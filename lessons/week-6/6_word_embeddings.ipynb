{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Word Embeddings\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand word embeddings, and how they are used in information retrieval.\n",
    "* Use `gensim`'s `word2vec` method to create word vectors for a corpus.\n",
    "* Use word embeddings to calculate word similarity.\n",
    "* Use word embeddings to reflect on implicit biases in your data.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí≠ **Reflection**: Reflecting on ethical implications, biases, and social impact in data science.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Word Embeddings](#we)\n",
    "2. [Using Comments Data](#comments)\n",
    "3. [Constructing a Word2Vec Model](#w2v)\n",
    "4. [Word Similarity](#sim)\n",
    "5. [Exploring Language Biases With Word Embeddings](#bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfjNP33MZOoi"
   },
   "source": [
    "<a id='we'></a>\n",
    "\n",
    "# Word Embeddings\n",
    "\n",
    "In this notebook, we'll work with word embeddings using `gensim` and Word2Vec.\n",
    "\n",
    "The goal of word embedding models is to learn **numerical representations** of text corpora. Like with TF-IDF, it's all about how we construct that numerical representation.\n",
    "\n",
    "When using word embeddings, we look for a **vector** of numbers to represent each term. The actual numbers themselves won't be meaningful to us as humans. However, if successful, the vectors for each term should encode information about the meaning or concept the term represents, as well as the relationship between it and other terms in the vocabulary.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "**Word2Vec** is one example of a word embeddings model. It learns by taking words and their contexts (e.g. sentences) into account, and can then try to predict other words. Given enough data, usage and contexts, Word2Vec can make accurate guesses about a word‚Äôs meaning based on its appearances. Those guesses can be used to establish a word‚Äôs association with other words (e.g. \"Paris\" is to \"France\" as ‚ÄúBerlin‚Äù is to ‚ÄúGermany‚Äù), or cluster documents and classify them by topic.\n",
    "\n",
    "<img src=\"../../images/we.png\" alt=\"Word Embeddings\" width=\"600\"/>\n",
    "\n",
    "Word vector models such as Word2Vec are fully **unsupervised**: they learn all of these meanings and relationships without any advance knowledge. Unsupervised learning requires the specification of a right task. We won't go into detail in this lesson, but you can roughly think of the task as predicting nearby words, given a specific word. \n",
    "\n",
    "Read [this post](https://tomvannuenen.medium.com/analyzing-reddit-communities-with-python-part-6-word-embeddings-f92bba876d60) if you want a deeper introduction to word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comments'></a>\n",
    "\n",
    "# Using Comments Data\n",
    "\n",
    "As we will be considering the language biases in the next notebook, we will use the **comments** of the AITA subreddit this time. The thinking behind this is that this data will be derived from more people, and include more evaluative statements (after all, comments on r/amitheasshole generally evaluate the original posts).\n",
    "\n",
    "üí° **Tip**: Whether you want to be using submissions or comments for your own data depends on the question you are asking of that data! Make sure to think about this.\n",
    "\n",
    "We have done the preprocessing for you and saved the trigrams (a list of lists) in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/aita_comment_trigrams.pickle', 'rb') as f:\n",
    "    # Load the object from the file\n",
    "    comments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into lists of words for Word2Vec\n",
    "comment_list = [comment.split() for comment in comments]\n",
    "comment_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk2mxX1HZOpk"
   },
   "source": [
    "<a id='w2v'></a>\n",
    "\n",
    "# Constructing a Word2Vec Model\n",
    "\n",
    "Let's create our word embeddings model. The input to this model is our corpus split up into words. The model's output is a set of \"vectors\" (one for each word) in $N$ dimensions (we choose $N$ before creating the model). Think of these vectors as \"features\", capturing latent meaning.\n",
    "\n",
    "This model allows us to group the vectors of similar words together in vector space. We can then reduce the dimensionality to visualize the results in a way humans can understand (such as in a 2-dimensional space), or to perform linear algebra operations in order to find out to what extent words are related.\n",
    "\n",
    "We now instantiate and train our Word2Vec model, using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQq117-pZOpl"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Number of cores at your disposal\n",
    "\n",
    "n_features = 300     # Word vector dimensionality (how many features each word will be given)\n",
    "min_word_count = 10  # Minimum word count to be taken into account\n",
    "n_workers = cores    # Number of threads to run in parallel (equal to your amount of cores)\n",
    "window = 5           # Context window size\n",
    "downsampling = 1e-2  # Downsample setting for frequent words\n",
    "seed = 1             # Seed for the random number generator (to create reproducible results)\n",
    "sg = 1               # Skip-gram = 1, CBOW = 0\n",
    "epochs = 20          # Number of iterations over the corpus\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=comment_list,\n",
    "    workers=n_workers,\n",
    "    vector_size=n_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window,\n",
    "    sample=downsampling,\n",
    "    seed=seed,\n",
    "    sg=sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrNV3eYeZOpo"
   },
   "source": [
    "That was it! We have a Word Embeddings model now. Let's save it so that we don't have to train it again. Then, we can reload the embeddings so that we don't have to train it every single time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRX-lzwSzFI5"
   },
   "outputs": [],
   "source": [
    "model.save('../../data/aita_comments.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZMDwWBuGTn7"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('../../data/aita_comments.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWQC4S9IZOpu"
   },
   "source": [
    "How many terms are in our vocabulary? Whenever interacting with the word vector dictionary, we use the `wv` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVdNakwKfm3H"
   },
   "source": [
    "Let's take a peek at the word vectors our model has learned. We can take a look at the individual words using the `index_to_key` attribute, and the word vectors themselves can be accessed with the `vectors` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1640016826009,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "R6A19I38fjTT",
    "outputId": "e6d18f9d-cd53-4ae6-d17e-7cf353618735"
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at it won't make a whole lot of sense to us! It's just a bunch of numbers. However, we can do semantic operations on these vectors, such as getting related terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw4th__0ZOp0"
   },
   "source": [
    "<a id='sim'></a>\n",
    "\n",
    "# Word Similarity\n",
    "\n",
    "With the information in our word embeddings model, we can try to find similarities between words that interest us (i.e. words that have a similar vector). Let's create a function that retrieves related terms to some input. We're going to use the `most_similar()` function in `gensim` as part of this helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HVouFqZOp0"
   },
   "outputs": [],
   "source": [
    "def get_most_similar_terms(model, token, topn=20):\n",
    "    \"\"\"Look up the top N most similar terms to the token.\"\"\"\n",
    "    for word, similarity in model.wv.most_similar(positive=[token], topn=topn):\n",
    "        print(f\"{word}: {round(similarity, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1639950052918,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "CDyV-ntGZOp3",
    "outputId": "d72277e1-e4e1-451d-e706-70b7fb7ebb02"
   },
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'asshole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other terms. What else interests you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'empathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing High Dimensional Spaces with $t$-SNE\n",
    "\n",
    "The word embeddings we created are what's called a **high-dimensional representation** of the text. That is, we take a word in the corpus, and represent it using, in this case, 300 numbers. We can plot 3 numbers at a time - that's in 3 dimensions - but there's no way for humans to visualize something in a 300-dimensional space. \n",
    "\n",
    "So, **dimensionality reduction** is a big part of machine learning. How can we take vectors that are 300-dimensional, and visualize them in 2-dimensions, while keeping the structure between vectors the same? How can we reduce the dimensionality?\n",
    "\n",
    "One of the most popular methods for dimensionality reduction is called $t$-SNE ($t$-Distributed Stochastic Neighbor Embedding). If you want to read more about this algorithm, [here](https://lvdmaaten.github.io/tsne/) is a good starting point. \n",
    "\n",
    "Roughly, $t$-SNE tries to keep the relative distances between points as closely as possible in both high-dimensional and low-dimensional space. We can thus visualize our embeddings, which may reveal semantic and syntactic trends in the data.\n",
    "\n",
    "One thing we can do is to look for relationships between particular word pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['asshole', 'jerk', 'rude', 'inconsiderate',\n",
    "         'angry', 'upset', 'sad', 'happy', 'frustrated',\n",
    "         'friend', 'family', 'partner', 'coworker', 'neighbor',\n",
    "         'apologize', 'forgive', 'ignore', 'confront', 'compromise',\n",
    "         'right', 'wrong', 'justified', 'unjustified', 'fair', 'unfair',\n",
    "         'honesty', 'loyalty', 'respect', 'kindness', 'empathy']\n",
    "\n",
    "# Extract the word vectors\n",
    "word_vectors = np.array([model.wv[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an ImportError in the line tsne=TSNE(), you might need to install scikit-learn:\n",
    "# %pip install -U scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=2, perplexity=2)\n",
    "reduced_vectors = tsne.fit_transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the t-SNE vectors\n",
    "words_df = pd.DataFrame(reduced_vectors,\n",
    "                            index=pd.Index([word for word in words]),\n",
    "                            columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LabelSet\n",
    "\n",
    "# Add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(words_df)\n",
    "\n",
    "# Create the plot and configure the title, dimensions, and tools\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings')\n",
    "\n",
    "# Add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips='@index'))\n",
    "\n",
    "# Draw the words as circles on the plot\n",
    "tsne_plot.circle('x', 'y',\n",
    "                 source=plot_data,\n",
    "                 color='blue',\n",
    "                 size=10,\n",
    "                 hover_line_color='black')\n",
    "\n",
    "# Add labels to the points\n",
    "labels = LabelSet(x='x', y='y', text='index', level='glyph',\n",
    "                  x_offset=5, y_offset=5, source=plot_data,\n",
    "                  render_mode='canvas')\n",
    "tsne_plot.add_layout(labels)\n",
    "\n",
    "# Engage!\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EF6bB0rVReh"
   },
   "source": [
    "Now let's use $t$-SNE to take **all** the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(init='pca', learning_rate='auto')\n",
    "X_tsne = tsne.fit_transform(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our low dimensional representation. Now, let's store the 2 dimensions in a dataframe, with the word as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the t-SNE vectors\n",
    "tsne_df = pd.DataFrame(X_tsne,\n",
    "                            index=pd.Index(model.wv.index_to_key),\n",
    "                            columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd60bqFxe9z1"
   },
   "outputs": [],
   "source": [
    "# Create some filepaths to save our model\n",
    "tsne_path = '../../data/tsne_model'\n",
    "tsne_df_path = '../../data/tsne_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBe7lNE3e7oQ"
   },
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(tsne_path, 'wb') as f:\n",
    "    pickle.dump(X_tsne, f)\n",
    "\n",
    "tsne_df.to_pickle(tsne_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a convenient code block to load this data, to start from this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tsne_path, 'rb') as f:\n",
    "    X_tsne = pickle.load(f)\n",
    "    \n",
    "tsne_df = pd.read_pickle(tsne_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to visualize the 2-dimensional space using a package called `bokeh`. This package is nice for this because it allows for some degree of interactivity: we can go over each point and dynamically get information about the word denoting that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AE4-OrmKmKG"
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "\n",
    "output_notebook()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1640017607454,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "myJtActVdjn4",
    "outputId": "5f283a90-c0f4-4c31-88ea-5f7bfa6917ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_df)\n",
    "\n",
    "# Create the plot and configure the title, dimensions, and tools\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings')\n",
    "\n",
    "# Add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips='@index') )\n",
    "\n",
    "# Draw the words as circles on the plot\n",
    "tsne_plot.circle('x', 'y',\n",
    "                 source=plot_data,\n",
    "                 color='blue',\n",
    "                 line_alpha=0.2,\n",
    "                 fill_alpha=0.1,\n",
    "                 size=10,\n",
    "                 hover_line_color='black')\n",
    "\n",
    "# Engage!\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtIBMFnNrQlL"
   },
   "source": [
    "<a id='bias'></a>\n",
    "\n",
    "# Exploring Language Biases With Word Embeddings\n",
    "\n",
    "Language carries implicit biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. \n",
    "\n",
    "One way to discover language biases is via word embeddings. In order to do so, we first need to postulate concepts for terms along which we might expect biases to exist. For example, humans often associate words with masculinity or femininity, which reflects both a normative construction of a gender binary, as well as stereotypes *within* that binary. We can attempt to quantify this directly using notions of distance with word embeddings.\n",
    "\n",
    "We first define **target concepts** for the terms we aim to identify biases towards (e.g., \"male\" and \"female\"). Then, we can then compute relative similarities of other word vectors ‚Äì particularly, words that act as evaluative attributes such as \"strong\" and \"sensitive\". These words can be categorised through clustering algorithms and labeled through a semantic analysis system into more general (conceptual) biases, yielding a broad picture of the biases present in a discourse community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSbUVbC-rQlY"
   },
   "source": [
    "## Obtaining Biases Towards Target Concepts\n",
    "\n",
    "We will run our method of finding biased words towards our target sets. We will be using functions in an external file - `utils.py` - which you are free to look through if you're interested. Otherwise, feel free to use the functions as desired.\n",
    "\n",
    "Given a vocabulary and two sets of target words (such as, in this case, those for *women* and *men*), we rank the words from least to most biased. As such, we obtain two ordered lists of the most biased words towards each target set, obtaining an overall view of the bias distribution in that particular community with respect to those two target sets. \n",
    "\n",
    "Here's what happening:\n",
    "- We calculate the centroid of a target set by averaging the embedding vectors in our target set (e.g. the vectors for `he, son, his, him, father, male` for our target concept `male`);\n",
    "- We calculate the cosine similarity between the vectors for all words in our vocabulary as compared to our two centroids (we also apply POS-filtering to only work with parts of speech we expect to be relevant);\n",
    "- We use a threshold based on standard deviation to determine how severe a bias needs to be before we include it;\n",
    "- We rank the words in the vocabulary of our word embeddings model based on their bias towards either target concept.\n",
    "\n",
    "The implicit hypothesis here is that word vectors closer to the centroid of a target concept are used more similarly with respect that concept. This, in effect, is an effort to quantify the bias of words in the discourse of a community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to calculate biased words\n",
    "from utils import calculate_biased_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our target concept vectors. This is a critical choice: the centroid of these represents the concept at hand. What happens if you change the words included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = ['sister' , 'female' , 'woman' , 'girl' , 'daughter' , 'she' , 'hers' , 'her']\n",
    "target2 = ['brother' , 'male' , 'man' , 'boy' , 'son' , 'he' , 'his' , 'him'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('../../data/aita_comments.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11526,
     "status": "ok",
     "timestamp": 1647303428120,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "SIzdtQV5rQlh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[b1, b2] = calculate_biased_words(model, target1, target2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJpg5BDydlYT"
   },
   "source": [
    "Let's print some biases. Here you see the most-biased words towards our target concepts (1 being *women*, 2 being *men*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1647303438347,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qc-9g8BErQll",
    "outputId": "93aa1d97-e9ad-4f17-9ce3-bef7d4079209"
   },
   "outputs": [],
   "source": [
    "print('Biased words towards target set 1')\n",
    "print([word for word in b1.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Biased words towards target set 2')\n",
    "print([word for word in b2.keys()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpk0lx2xplMf"
   },
   "source": [
    "## Visualizing Biases using $t$-SNE\n",
    "\n",
    "We now return to our dimensionality reduction technique, $t$-SNE, to try and visualize these biased words in a 2D space using Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tsne_path, 'rb') as f:\n",
    "    X_tsne = pickle.load(f)\n",
    "    \n",
    "tsne_df = pd.read_pickle(tsne_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert biased term keys to arrays\n",
    "target1_idx = np.array([model.wv.key_to_index[key] for key in b1.keys()])\n",
    "target2_idx = np.array([model.wv.key_to_index[key] for key in b2.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find t-sne values for the biased sets\n",
    "X_target1 = X_tsne[target1_idx]\n",
    "X_target2 = X_tsne[target2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "\n",
    "# Set up the Bokeh plot\n",
    "output_notebook()\n",
    "\n",
    "p = figure()\n",
    "\n",
    "# Create ColumnDataSource for X_target1 (blue)\n",
    "source1 = ColumnDataSource(data=dict(x=X_target1[:, 0], y=X_target1[:, 1], label=[model.wv.index_to_key[idx] for idx in target1_idx]))\n",
    "\n",
    "# Create ColumnDataSource for X_target2 (red)\n",
    "source2 = ColumnDataSource(data=dict(x=X_target2[:, 0], y=X_target2[:, 1], label=[model.wv.index_to_key[idx] for idx in target2_idx]))\n",
    "\n",
    "# Add scatter plot for X_target1 (blue)\n",
    "p.scatter(x='x', y='y', color='blue', size=8, source=source1)\n",
    "\n",
    "# Add scatter plot for X_target2 (red)\n",
    "p.scatter(x='x', y='y', color='red', size=8, source=source2)\n",
    "\n",
    "# Add labels for X_target1\n",
    "labels1 = LabelSet(x='x', y='y', text='label', x_offset=6, y_offset=3, source=source1, render_mode='canvas')\n",
    "p.add_layout(labels1)\n",
    "\n",
    "# Add labels for X_target2\n",
    "labels2 = LabelSet(x='x', y='y', text='label', x_offset=6, y_offset=3, source=source2, render_mode='canvas')\n",
    "p.add_layout(labels2)\n",
    "\n",
    "# Show the plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ Reflection\n",
    "\n",
    "Note that these binary target concepts are often a product of ideology and normativity in society: the gender binary is a good example. When checking for biases towards certain concepts, make sure you consider the fact that you are the one creating / reproducing these concepts, and that you may be reinforcing a constructed binary!\n",
    "\n",
    "Also note that determining your own target concepts and biases is a **iterative** process. Try changing some of the words in the target concepts to see how the biased words and plot change, and discuss with your group what you think makes for a coherent and robust target set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riINX9wVyFRJ",
    "tags": []
   },
   "source": [
    "## Existing Target Sets\n",
    "\n",
    "Here are some other target sets that have been previously used in the literature:\n",
    "\n",
    "* *Gender target sets taken from Nosek, Banaji, and Greenwald 2002.*\n",
    "    - Female: `sister, female, woman, girl, daughter, she, hers, her`.\n",
    "    - Male: `brother, male, man, boy, son, he, his, him`.\n",
    "* *Religion target sets taken from Garg et al. 2018.*\n",
    "    - Islam: `allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad`.\n",
    "    - Christianity: `baptism, messiah, catholicism, resurrection, christianity, salva-tion, protestant, gospel, trinity, jesus, christ, christian, cross,catholic, church`.\n",
    "* *Racial target sets taken from Garg et al. 2017*\n",
    "    - White last names: `harris, nelson, robinson, thompson, moore, wright, anderson, clark, jackson, taylor, scott, davis, allen, adams, lewis, williams, jones, wilson, martin, johnson`.\n",
    "    - Hispanic last names: `ruiz, alvarez, vargas, castillo, gomez, soto,gonzalez, sanchez, rivera, mendoza, martinez, torres, ro-driguez, perez, lopez, medina, diaz, garcia, castro, cruz`.\n",
    "    - Asian last names: `cho, wong, tang, huang, chu, chung, ng,wu, liu, chen, lin, yang, kim, chang, shah, wang, li, khan,singh, hong`.\n",
    "    - Russian last names: `gurin, minsky, sokolov, markov, maslow, novikoff, mishkin, smirnov, orloff, ivanov, sokoloff, davidoff, savin, romanoff, babinski, sorokin, levin, pavlov, rodin, agin`.\n",
    "* *Career/family target sets taken from Garg et al. 2018.*\n",
    "    - Career: `executive, management, professional, corporation, salary, office, business, career`.\n",
    "    - Family: `home, parents, children, family, cousins, marriage, wedding, relatives.Math: math, algebra, geometry, calculus, equations, computation, numbers, addition`.\n",
    "* *Arts/Science target sets taken from Garg et al. 2018.*\n",
    "    - Arts: `poetry, art, sculpture, dance, literature, novel, symphony, drama`.\n",
    "    - Science: `science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHCHMAPYTCVX"
   },
   "source": [
    "### Sources\n",
    "\n",
    "Nosek, B. A., Banaji, M. R., & Greenwald, A. G. (2002). Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics, 6(1), 101‚Äì115. https://doi.org/10.1037/1089-2699.6.1.101\n",
    "\n",
    "Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2017). Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes, 1‚Äì33."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 4-1 Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
